{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping com Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A extração de dados da web, ou web scraping, é o processo automatizado de coletar informações de websites. O que começou como simples scripts para extrair conteúdo de arquivos HTML estáticos evoluiu para sofisticadas automações de navegador e interações com APIs complexas. \n",
    "\n",
    "Hoje, a extração de dados é um pilar fundamental para a tomada de decisões em negócios, análise de mercado, pesquisa acadêmica e, cada vez mais, para o treinamento de modelos de Inteligência Artificial.\n",
    "\n",
    "A evolução da própria web, de um conjunto de documentos estáticos para aplicações interativas e dinâmicas (Single-Page Applications - SPAs), impulsionou uma especialização correspondente nas ferramentas de scraping. \n",
    "\n",
    "Ferramentas que simplesmente fazem requisições HTTP, como a `requests`, são extremamente eficientes para sites tradicionais, mas insuficientes para páginas que dependem de JavaScript para renderizar seu conteúdo. \n",
    "\n",
    "Isso levou ao desenvolvimento de ferramentas de automação de navegador, como `Selenium` e `Playwright`, que controlam um navegador real para interagir com essas aplicações complexas.\n",
    "\n",
    "Este fenômeno implica que a escolha de uma ferramenta de web scraping deixou de ser uma mera preferência técnica para se tornar um diagnóstico estratégico. A primeira e mais crucial etapa de qualquer projeto de scraping moderno é a análise do site-alvo. \n",
    "\n",
    "É preciso determinar sua arquitetura: é um site estático? É uma SPA que renderiza dados no lado do cliente? Existe uma API interna que pode ser acessada diretamente? \n",
    "\n",
    "A resposta a essas perguntas ditará a abordagem mais eficiente e robusta. Este guia não apenas ensinará como usar as ferramentas, mas também como desenvolver essa mentalidade de \"diagnosticar antes de prescrever\", garantindo que você escolha a ferramenta certa para cada trabalho, otimizando performance, tempo de desenvolvimento e a longevidade do seu scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Jornada de Aprendizado\n",
    "\n",
    "Este material foi estruturado para guiá-lo em uma jornada de aprendizado progressiva, desde os conceitos fundamentais até as técnicas mais avançadas e as considerações profissionais. O nosso roteiro será:\n",
    "\n",
    "1.  **Fundamentos com `Requests` e `BeautifulSoup`:** A base para extrair dados de sites estáticos.\n",
    "2.  **Lidando com a web moderna e dinâmica com `Selenium` e `Playwright`:** Ferramentas para controlar navegadores e extrair conteúdo renderizado por JavaScript.\n",
    "3.  **Escalando projetos com o framework `Scrapy`:** Uma solução completa para projetos de extração de dados em larga escala.\n",
    "4.  **A alternativa profissional: o uso de APIs:** A forma mais robusta e ética de obter dados, utilizando bibliotecas `wrapper` para serviços populares.\n",
    "5.  **Técnicas avançadas para robustez:** Estratégias para lidar com logins, paginação e evasão de bloqueios.\n",
    "6.  **A base indispensável: ética e legalidade:** As regras e boas práticas que garantem que seu trabalho seja responsável e sustentável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 1: Fundamentos do Scraping - A Dupla Clássica: `Requests` e `BeautifulSoup`\n",
    "\n",
    "Para a vasta maioria dos sites mais simples, cujo conteúdo é entregue diretamente no HTML inicial, a combinação das bibliotecas `requests` e `BeautifulSoup` é a abordagem mais rápida e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Protocolo HTTP em Ação com `requests`\n",
    "\n",
    "A biblioteca `requests` é o padrão de fato para realizar requisições HTTP em Python. Sua popularidade deriva de uma API elegante e simples que abstrai as complexidades do protocolo HTTP, tornando a interação com a web uma tarefa intuitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código Prático: Fazendo uma Requisição `GET`**\n",
    "\n",
    "O método mais comum é o `GET`, usado para solicitar dados de um recurso específico. Para obter o conteúdo de uma página, basta passar sua URL para a função `requests.get()`:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''url = 'https://www.example.com'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mresponse = requests.get(url)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03melse:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    print(f\"Falha na requisição. Código de status: {response.status_code}\")'''\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "'''url = 'https://www.example.com'\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.example.com'\n",
>>>>>>> a5e8c23b (Adicionando os notebooks da aula do dia 10/09, sobre coleta de dados via api e scrapt)
    "response = requests.get(url)\n",
    "\n",
    "# Verificar se a requisição foi bem-sucedida\n",
    "if response.status_code == 200:\n",
    "    print(\"Requisição bem-sucedida!\")\n",
    "    # print(response.text) # Exibe o conteúdo HTML da página\n",
    "else:\n",
<<<<<<< HEAD
    "    print(f\"Falha na requisição. Código de status: {response.status_code}\")'''"
=======
    "    print(f\"Falha na requisição. Código de status: {response.status_code}\")"
>>>>>>> a5e8c23b (Adicionando os notebooks da aula do dia 10/09, sobre coleta de dados via api e scrapt)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O atributo `response.status_code` é fundamental para verificar a saúde da nossa requisição. Um código `200` indica sucesso, enquanto códigos na faixa de 400 (erros do cliente) ou 500 (erros do servidor) sinalizam problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explorando a Resposta**\n",
    "\n",
    "O objeto `response` contém todas as informações retornadas pelo servidor. Os dois atributos mais importantes para o scraping são:\n",
    "* `response.text`: Retorna o conteúdo da resposta como uma string decodificada (Unicode). O `requests` tenta adivinhar a codificação correta com base nos cabeçalhos HTTP.[13, 17]\n",
    "* `response.content`: Retorna o conteúdo da resposta como bytes brutos. Isso é útil quando a decodificação automática do `.text` falha ou quando estamos lidando com dados não textuais, como imagens.[13, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parâmetros e Cabeçalhos**\n",
    "\n",
    "Muitas vezes, precisamos personalizar nossas requisições.\n",
    "* **Parâmetros de URL:** Para interagir com funcionalidades de busca ou filtros, podemos passar um dicionário de parâmetros para o argumento `params`. A biblioteca `requests` cuidará de codificá-los corretamente na URL."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exemplo de busca na API do GitHub por repositórios Python populares\u001b[39;00m\n\u001b[1;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage:python\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m response_api \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.github.com/search/repositories\u001b[39m\u001b[38;5;124m'\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# A URL final será algo como: https://api.github.com/search/repositories?q=language%3Apython&sort=stars&order=desc\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL da requisição: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_api\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> a5e8c23b (Adicionando os notebooks da aula do dia 10/09, sobre coleta de dados via api e scrapt)
   "source": [
    "# Exemplo de busca na API do GitHub por repositórios Python populares\n",
    "params = {\n",
    "    'q': 'language:python',\n",
    "    'sort': 'stars',\n",
    "    'order': 'desc'\n",
    "}\n",
    "response_api = requests.get('https://api.github.com/search/repositories', params=params)\n",
    "# A URL final será algo como: https://api.github.com/search/repositories?q=language%3Apython&sort=stars&order=desc\n",
    "print(f\"URL da requisição: {response_api.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Cabeçalhos HTTP (Headers):** Os cabeçalhos enviam metadados sobre a requisição. Um dos mais importantes para web scraping é o `User-Agent`. Por padrão, o `requests` se identifica como `python-requests/versão`, o que torna trivial para um site identificar e bloquear um scraper. Modificar o `User-Agent` para simular um navegador comum é uma primeira linha de defesa essencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "response_com_header = requests.get(url, headers=headers)\n",
    "print(f\"Status com header modificado: {response_com_header.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decifrando o HTML com `BeautifulSoup`\n",
    "\n",
    "Uma vez que tenhamos o conteúdo HTML bruto, precisamos de uma forma de analisá-lo (fazer o *parsing*). O HTML é uma linguagem de marcação que estrutura o conteúdo de uma página, mas como texto puro, é difícil de navegar. A biblioteca `BeautifulSoup` resolve isso transformando o HTML em uma árvore de objetos Python, que podemos percorrer e consultar de forma programática.[2, 20, 21, 22]\n",
    "\n",
    "Para começar, instale a biblioteca e um parser. O `lxml` é altamente recomendado por sua velocidade e robustez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, crie um objeto `BeautifulSoup` (comumente chamado de `soup`) a partir do conteúdo da resposta da requisição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Supondo que 'response' seja o objeto de uma requisição bem-sucedida\n",
    "response = response_com_header\n",
    "soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "# A função .prettify() ajuda a visualizar a estrutura aninhada do HTML\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navegação na Árvore DOM**\n",
    "\n",
    "O `BeautifulSoup` oferece uma API intuitiva para encontrar os elementos de que precisamos:\n",
    "* `soup.find('tag_name')`: Retorna a **primeira** ocorrência de uma tag.\n",
    "* `soup.find_all('tag_name')`: Retorna uma **lista** com todas as ocorrências de uma tag.\n",
    "* **Acessando Atributos:** Uma vez que você tem um objeto de tag, pode acessar seus atributos como um dicionário. Por exemplo, `link.get('href')` ou `link['href']` para obter o URL de uma tag `<a>`.\n",
    "* **Extraindo Texto:** Use `.text` ou `.get_text()` para extrair o conteúdo textual de uma tag e de todas as suas tags filhas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Arte da Seleção: Seletores CSS vs. XPath\n",
    "\n",
    "Enquanto `find()` e `find_all()` são úteis para buscas simples, projetos mais complexos exigem uma forma mais precisa de selecionar elementos. As duas linguagens padrão para isso são os Seletores CSS e o XPath.\n",
    "* **Seletores CSS:** Esta é a mesma sintaxe usada por desenvolvedores web para aplicar estilos a páginas HTML. Sua sintaxe é concisa e intuitiva, permitindo selecionar elementos por tag, classe (`.`), ID (`#`), atributos e suas relações hierárquicas (descendentes, filhos diretos). No `BeautifulSoup`, você os utiliza com os métodos `.select_one()` (retorna o primeiro) e `.select()` (retorna uma lista).\n",
    "\n",
    "* **XPath (XML Path Language):** É uma linguagem de consulta mais poderosa e flexível, originalmente projetada para documentos XML. Sua principal vantagem sobre os seletores CSS é a capacidade de navegar em **qualquer direção** na árvore DOM, incluindo para cima (selecionar elementos pais) e lateralmente (selecionar irmãos). Além disso, o XPath pode selecionar elementos com base no seu conteúdo de texto, uma funcionalidade extremamente útil que falta nos seletores CSS.\n",
    "\n",
    "A escolha entre os dois envolve um trade-off. Os seletores CSS são geralmente mais rápidos, mais fáceis de ler e suficientes para a maioria dos casos. O XPath, embora mais verboso, oferece um poder inigualável para navegar em estruturas HTML complexas ou malformadas. A prática recomendada é usar seletores CSS como padrão e recorrer ao XPath apenas quando suas capacidades avançadas forem necessárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Característica | Seletores CSS | XPath |\n",
    "| :--- | :--- | :--- |\n",
    "| **Sintaxe** | Concisa e intuitiva (ex: `div#main p.content`) | Mais verbosa, baseada em caminhos (ex: `//div[@id='main']/p[@class='content']`) |\n",
    "| **Velocidade** | Geralmente mais rápido, otimizado nativamente pelos parsers | Ligeiramente mais lento devido à sua complexidade  |\n",
    "| **Navegação DOM** | Apenas descendente (de pai para filho)  | Bidirecional (pode selecionar pais, ancestrais e irmãos)  |\n",
    "| **Seleção por Texto** | Não suportado nativamente (requer pós-processamento) | Suportado nativamente com funções como `contains(text(), '...')`  |\n",
    "| **Suporte** | Amplo suporte em todas as principais bibliotecas de parsing | Suporte bom, mas às vezes menos proeminente que CSS  |\n",
    "| **Legibilidade** | Alta, especialmente para seletores simples  | Menor, pode se tornar complexo rapidamente  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício Prático 1: Scraper de Citações Estático\n",
    "\n",
    "Vamos construir um scraper completo para o site `http://quotes.toscrape.com`, que é uma excelente plataforma de testes para iniciantes.\n",
    "\n",
    "**Objetivo:** Extrair o texto, o autor e as tags de cada citação na primeira página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# 1. Obter o HTML\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 2. Criar o objeto BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Lista para armazenar os dados extraídos\n",
    "quotes_data = []\n",
    "\n",
    "# 3. Inspecionar a página e encontrar os seletores\n",
    "# O seletor para cada bloco de citação é 'div.quote'\n",
    "quote_elements = soup.select('div.quote')\n",
    "\n",
    "# 4. Iterar e extrair os dados\n",
    "for quote_element in quote_elements:\n",
    "    # Seletor para o texto da citação: 'span.text'\n",
    "    text = quote_element.select_one('span.text').text\n",
    "    \n",
    "    # Seletor para o autor: 'small.author'\n",
    "    author = quote_element.select_one('small.author').text\n",
    "    \n",
    "    # Seletor para as tags: 'a.tag'\n",
    "    tags_elements = quote_element.select('a.tag')\n",
    "    tags = [tag.text for tag in tags_elements]\n",
    "    \n",
    "    quotes_data.append({\n",
    "        'texto': text,\n",
    "        'autor': author,\n",
    "        'tags': tags\n",
    "    })\n",
    "\n",
    "# 5. Salvar os dados em um arquivo CSV\n",
    "if quotes_data:\n",
    "    keys = quotes_data[0].keys()\n",
    "    with open('citacoes.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(quotes_data)\n",
    "    print(\"Dados salvos com sucesso em citacoes.csv\")\n",
    "\n",
    "print(quotes_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 2: A Web Dinâmica - Dominando JavaScript com `Selenium` e `Playwright`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Desafio do Conteúdo Renderizado no Cliente\n",
    "\n",
    "A abordagem com `requests` e `BeautifulSoup` falha em um número crescente de sites modernos. \n",
    "\n",
    "Aplicações de página única (SPAs), construídas com frameworks como React, Angular ou Vue.js, frequentemente carregam um esqueleto HTML mínimo e, em seguida, utilizam JavaScript para buscar dados de um servidor e renderizar o conteúdo dinamicamente no navegador do cliente.\n",
    "\n",
    "Se usarmos `requests` em tal site, obteremos apenas o HTML inicial, vazio do conteúdo que realmente nos interessa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automação de Navegadores: Uma Visão Geral\n",
    "\n",
    "Para extrair dados desses sites, precisamos de uma ferramenta que possa executar o JavaScript da mesma forma que um navegador real. É aqui que entram as bibliotecas de automação de navegador. \n",
    "\n",
    "Elas nos permitem controlar programaticamente um navegador como Chrome, Firefox ou Safari, simulando interações humanas como cliques, rolagem de página e preenchimento de formulários. \n",
    "\n",
    "Ao fazer isso, damos tempo para que o JavaScript seja executado, a página seja totalmente renderizada e, só então, extraímos o HTML final para análise.\n",
    "\n",
    "As duas principais ferramentas para esta tarefa em Python são `Selenium` e `Playwright`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Comparativa Aprofundada (2025): `Selenium` vs. `Playwright`\n",
    "\n",
    "A escolha entre `Selenium` e `Playwright` tornou-se um ponto central de discussão na comunidade de automação e web scraping. Embora ambos atinjam o mesmo objetivo fundamental, suas arquiteturas, APIs e filosofias são distintas.\n",
    "\n",
    "#### **`Selenium`:** \n",
    "É a ferramenta veterana e estabelecida, lançada originalmente para testes automatizados de software. Sua longevidade lhe confere um ecossistema vasto, com uma comunidade enorme, inúmeros tutoriais e suporte para quase todas as linguagens de programação e navegadores, incluindo versões mais antigas.\n",
    "\n",
    "No entanto, sua arquitetura mais antiga pode resultar em performance mais lenta e maior propensão a instabilidade (\"flakiness\"). \n",
    "\n",
    "Uma das principais críticas é a necessidade de o desenvolvedor gerenciar manualmente as esperas (`waits`), escrevendo código explícito para aguardar que os elementos apareçam na página antes de interagir com eles, o que pode tornar os scripts mais verbosos e frágeis.\n",
    "\n",
    "#### **`Playwright`:**\n",
    "Lançado pela Microsoft em 2020, o `Playwright` é uma alternativa moderna projetada para superar muitas das limitações do `Selenium`. \n",
    "\n",
    "Sua arquitetura é inerentemente mais rápida e robusta. Um de seus recursos mais elogiados é o **`auto-waiting`**: o `Playwright` espera automaticamente que os elementos estejam prontos para interação antes de executar uma ação, eliminando a necessidade de grande parte do código de espera manual.\n",
    "\n",
    "Além disso, introduz o conceito de **`Browser Contexts`**, que são sessões de navegador isoladas e leves. Isso permite um paralelismo muito mais eficiente do que o `Selenium`, que tradicionalmente precisa iniciar uma nova e pesada instância de navegador para cada sessão paralela.\n",
    "\n",
    "A ascensão do `Playwright` reflete uma tendência maior no desenvolvimento de software: a priorização da **Experiência do Desenvolvedor (DX)**. \n",
    "\n",
    "As frustrações comuns com a instabilidade e a verbosidade do `Selenium` criaram uma demanda por uma ferramenta que resolvesse esses problemas \"de fábrica\". Recursos como `auto-waits`, o gerador de código `Codegen` e a ferramenta de depuração `Inspector` tornam o `Playwright` mais fácil de configurar, mais intuitivo de usar e mais simples de depurar.\n",
    "\n",
    "Para o web scraping, isso significa que o tempo total de desenvolvimento e manutenção de um scraper está se tornando um fator tão importante quanto sua performance de execução. \n",
    "\n",
    "O `Playwright` otimiza o tempo do desenvolvedor, acelerando a curva de aprendizado e resultando em scripts mais limpos e robustos por padrão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Característica | Selenium | Playwright |\n",
    "| :--- | :--- | :--- |\n",
    "| **Performance** | Mais lento; arquitetura mais antiga baseada em WebDriver | Mais rápido; arquitetura moderna com `Browser Contexts` para paralelismo eficiente |\n",
    "| **Gestão de Esperas** | Manual; requer `WebDriverWait` e `expected_conditions` explícitos  | Automática (`auto-waits`); espera por elementos antes de agir, reduzindo \"flakiness\" |\n",
    "| **API e DX** | API mais verbosa; curva de aprendizado pode ser mais íngreme  | API mais concisa e moderna; ferramentas como `Codegen` e `Inspector` melhoram a DX  |\n",
    "| **Ecossistema** | Vasto e maduro; enorme comunidade e abundância de recursos | Mais novo, mas em rápido crescimento; excelente documentação oficial  |\n",
    "| **Suporte a Navegadores** | Suporte mais amplo, incluindo versões legadas e navegadores menos comuns  | Focado nos motores modernos: Chromium (Chrome, Edge), Firefox e WebKit (Safari)  |\n",
    "| **Detecção de Bots** | Mais facilmente detectável por padrão; requer bibliotecas adicionais para \"stealth\"  | Possui alguns recursos \"stealth\" integrados, tornando-o ligeiramente mais evasivo |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício Prático 2: Scraper de E-commerce com `Playwright`\n",
    "\n",
    "Vamos criar um scraper para uma página de e-commerce que utiliza rolagem infinita (\"infinite scroll\") para carregar mais produtos, um cenário clássico de conteúdo dinâmico.\n",
    "\n",
    "**Objetivo:** Extrair os nomes e preços de todos os produtos de uma página, rolando até o final para garantir que todos os itens sejam carregados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instale o Playwright e seus navegadores\n",
    "!pip install playwright\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from playwright.sync_api import sync_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def run(playwright):\n",
    "    # Inicia o navegador Chromium em modo headless (sem interface gráfica)\n",
    "    browser = playwright.chromium.launch(headless=True)\n",
    "    context = browser.new_context(\n",
    "        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "    )\n",
    "    page = context.new_page()\n",
    "    \n",
    "    # Navega para a URL\n",
    "    url = 'https://www.scrapingcourse.com/ecommerce/page/1/'\n",
    "    page.goto(url, wait_until='domcontentloaded')\n",
    "\n",
    "    # Lógica de rolagem infinita (neste caso, é paginação, mas a lógica é similar)\n",
    "    # Para rolagem infinita real, o loop seria baseado na altura da página\n",
    "    # Exemplo para paginação: encontrar e clicar no botão 'next'\n",
    "    \n",
    "    all_products = []\n",
    "    \n",
    "    while True:\n",
    "        # Extrai o conteúdo da página atual e passa para o BeautifulSoup\n",
    "        soup = BeautifulSoup(page.content(), 'lxml')\n",
    "        \n",
    "        products_on_page = soup.select('li.product')\n",
    "        for product in products_on_page:\n",
    "            name = product.select_one('h2.woocommerce-loop-product__title').text\n",
    "            price = product.select_one('span.price').text\n",
    "            all_products.append({'nome': name, 'preco': price})\n",
    "        \n",
    "        # Procura pelo link da próxima página\n",
    "        next_button = page.locator('a.next.page-numbers')\n",
    "        \n",
    "        # Se não houver botão 'next', quebra o loop\n",
    "        if not next_button.is_visible():\n",
    "            break\n",
    "            \n",
    "        # Clica no botão para ir para a próxima página\n",
    "        next_button.click()\n",
    "        # Espera a navegação para a próxima página ser concluída\n",
    "        page.wait_for_load_state('domcontentloaded')\n",
    "        time.sleep(1) # Pequeno delay para garantir que tudo carregue\n",
    "\n",
    "    # Fecha o navegador\n",
    "    browser.close()\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "with sync_playwright() as playwright:\n",
    "    products = run(playwright)\n",
    "    print(f\"Total de produtos extraídos: {len(products)}\")\n",
    "    if products:\n",
    "        print(\"Exemplo de produto:\", products[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo demonstra uma combinação poderosa: usar o `Playwright` para a automação do navegador (navegação, cliques, espera por conteúdo) e, em seguida, passar o HTML renderizado para o `BeautifulSoup` para uma análise e extração de dados mais fácil e familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 3: Para Grandes Projetos - Escalando com o Framework `Scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Além das Bibliotecas: Quando e por que usar um Framework\n",
    "\n",
    "Para tarefas simples, a combinação de `requests` com `BeautifulSoup` ou `Playwright` é suficiente. \n",
    "\n",
    "No entanto, quando os projetos crescem em escala — necessitando extrair dados de milhares de páginas, gerenciar múltiplas requisições concorrentes, processar os dados de forma estruturada e lidar com erros de forma robusta — um script simples se torna difícil de manter. \n",
    "\n",
    "Nesses casos, a utilização de um framework se torna indispensável.\n",
    "\n",
    "`Scrapy` é um framework de web crawling e scraping de código aberto, escrito em Python. Ele é \"batteries-included\", ou seja, vem com todas as ferramentas necessárias para construir crawlers complexos e eficientes. \n",
    "\n",
    "Sua arquitetura é baseada em requisições assíncronas, o que lhe permite realizar múltiplas requisições de rede em paralelo, resultando em uma performance muito superior para projetos de larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomia de um Projeto `Scrapy`\n",
    "\n",
    "Ao iniciar um projeto com o comando `scrapy startproject nome_do_projeto`, o `Scrapy` cria uma estrutura de diretórios organizada que separa as diferentes lógicas da aplicação. Os componentes principais são:\n",
    "\n",
    "* **Spiders:** São as classes Python onde você define a lógica de crawling. Um spider começa em uma ou mais URLs iniciais, define como seguir links para outras páginas e como extrair os dados de cada página visitada. Os componentes chave de um spider são o atributo `name`, a lista `start_urls` e o método `parse`.\n",
    "* **Items:** São como contêineres de dados. Você define um `scrapy.Item` com os campos que deseja extrair (ex: nome, preço, descrição). Isso ajuda a manter os dados organizados e consistentes.\n",
    "* **Item Pipelines:** Depois que um spider extrai um item, ele é enviado para o Item Pipeline. Aqui, você pode definir uma série de etapas de processamento sequenciais, como limpar os dados (remover HTML, formatar valores), verificar se há dados duplicados, e armazenar o item final em um banco de dados, arquivo CSV ou JSON.\n",
    "* **Middlewares:** São ganchos que se inserem no fluxo de requisições e respostas do `Scrapy`. Eles são extremamente poderosos para tarefas como modificar requisições antes de serem enviadas (ex: rotacionar `User-Agents` ou proxies) ou processar respostas antes que cheguem ao spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício Prático 3: Crawler de Notícias com `Scrapy`\n",
    "\n",
    "**Objetivo:** Criar um spider que navega por um portal de notícias, extrai os títulos e links dos artigos na página principal, segue cada link para a página do artigo e extrai o primeiro parágrafo do conteúdo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Crie o projeto e o spider:**\n",
    "```bash\n",
    "scrapy startproject portal_noticias\n",
    "cd portal_noticias\n",
    "scrapy genspider noticias quotes.toscrape.com \n",
    "```\n",
    "(Usaremos `quotes.toscrape.com` como exemplo, mas a lógica se aplica a um portal de notícias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Defina o Item em `items.py`:**\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ArtigoItem(scrapy.Item):\n",
    "    titulo = scrapy.Field()\n",
    "    autor = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Escreva o Spider em `spiders/noticias.py`:**\n",
    "```python\n",
    "import scrapy\n",
    "from portal_noticias.items import ArtigoItem\n",
    "\n",
    "class NoticiasSpider(scrapy.Spider):\n",
    "    name = 'noticias'\n",
    "    start_urls = ['[http://quotes.toscrape.com/](http://quotes.toscrape.com/)']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extrai informações de cada citação na página atual\n",
    "        for quote in response.css('div.quote'):\n",
    "            item = ArtigoItem()\n",
    "            item['titulo'] = quote.css('span.text::text').get()\n",
    "            item['autor'] = quote.css('small.author::text').get()\n",
    "            # O link para detalhes do autor pode ser usado como exemplo de \"link do artigo\"\n",
    "            author_page_link = quote.css('a::attr(href)').get()\n",
    "            item['link'] = response.urljoin(author_page_link)\n",
    "            yield item\n",
    "\n",
    "        # Lógica para seguir a paginação\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Execute o Spider e salve a saída:**\n",
    "```bash\n",
    "scrapy crawl noticias -o noticias.json\n",
    "```\n",
    "Este comando executará o spider, que navegará por todas as páginas do site, extrairá os dados e os salvará automaticamente no arquivo `noticias.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 4: A Rota Profissional - Consumindo Dados via APIs Oficiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping vs. APIs: Uma Análise de Trade-offs\n",
    "\n",
    "Embora o web scraping seja uma ferramenta poderosa, ele possui fragilidades inerentes. \n",
    "\n",
    "A estrutura de um site pode mudar a qualquer momento, quebrando seu scraper. \n",
    "\n",
    "Além disso, há sempre uma zona cinzenta em termos de legalidade e ética. \n",
    "\n",
    "Por essas razões, sempre que um site oferece uma API (Application Programming Interface) oficial, essa deve ser a sua primeira opção.\n",
    "\n",
    "Usar uma API é quase sempre preferível por três motivos principais:\n",
    "1.  **Confiabilidade:** As APIs fornecem dados já estruturados, geralmente em formato JSON, eliminando a necessidade de fazer o parsing de HTML. Isso torna seu código imensamente mais robusto a mudanças no layout do site.\n",
    "2.  **Eficiência:** Uma chamada de API busca apenas os dados de que você precisa, resultando em uma transferência de dados muito menor e mais rápida em comparação com o download e a renderização de uma página web inteira.\n",
    "3.  **Legalidade e Ética:** Ao usar uma API, você está operando dentro das regras e limites (como rate limits) estabelecidos pelo provedor. Isso remove a ambiguidade legal e garante que você não está sobrecarregando os servidores do serviço."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interagindo com APIs REST\n",
    "\n",
    "A maioria das APIs web modernas são APIs RESTful, que são acessadas através de requisições HTTP padrão. A biblioteca `requests` é a ferramenta perfeita para isso. \n",
    "\n",
    "A interação geralmente envolve enviar uma requisição `GET` para um *endpoint* (uma URL específica da API), possivelmente com parâmetros e um token de autenticação, e depois processar a resposta JSON recebida com o método `response.json()`.\n",
    "\n",
    "Uma técnica avançada, que une o mundo do scraping dinâmico e das APIs, é usar as ferramentas de automação de navegador não para extrair o HTML, mas para descobrir as APIs internas que um site usa. \n",
    "\n",
    "Muitas SPAs não têm APIs públicas documentadas, mas usam APIs internas para carregar seus próprios dados. \n",
    "\n",
    "Ao abrir as Ferramentas de Desenvolvedor do seu navegador (geralmente com F12) e ir para a aba \"Network\" (Rede), você pode observar as requisições `Fetch/XHR` que a página faz enquanto você interage com ela. \n",
    "\n",
    "Essas requisições são chamadas para a API interna. Ao identificar o endpoint e os parâmetros, você pode abandonar a automação do navegador (que é lenta e consome muitos recursos) e construir um scraper muito mais rápido e eficiente usando `requests` para chamar diretamente essa API, simulando o comportamento do front-end do site. \n",
    "\n",
    "Esta abordagem representa o auge da eficiência no scraping, combinando o melhor dos dois mundos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Caso 1: Coletando Dados do Reddit com `PRAW`\n",
    "\n",
    "O `PRAW` (Python Reddit API Wrapper) é uma biblioteca que simplifica enormemente a interação com a API do Reddit, abstraindo os detalhes das requisições HTTP e do fluxo de autenticação OAuth.[57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código Prático:** Extrair os 5 posts mais \"quentes\" (`hot`) do subreddit r/Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Crie uma instância do Reddit. É altamente recomendável usar um arquivo praw.ini\n",
    "# para armazenar suas credenciais de forma segura.\n",
    "# Veja a documentação do PRAW para mais detalhes.\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"SEU_CLIENT_ID\",\n",
    "        client_secret=\"SEU_CLIENT_SECRET\",\n",
    "        user_agent=\"MeuScraper/1.0 by u/SEU_USERNAME\"\n",
    "    )\n",
    "\n",
    "    # Acessa o subreddit\n",
    "    subreddit = reddit.subreddit('python')\n",
    "\n",
    "    print(f\"Posts mais quentes em r/{subreddit.display_name}:\")\n",
    "    # Itera sobre os posts\n",
    "    for submission in subreddit.hot(limit=5):\n",
    "        print(f\"Título: {submission.title}\")\n",
    "        print(f\"Score: {submission.score}\")\n",
    "        print(f\"URL: {submission.url}\")\n",
    "        print(\"-\" * 20)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao acessar a API do Reddit. Verifique suas credenciais. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Caso 2: Acessando Músicas no Spotify com `Spotipy`\n",
    "\n",
    "`Spotipy` é a biblioteca de escolha para interagir com a Web API do Spotify, facilitando a busca por músicas, artistas, álbuns e playlists.[60, 61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código Prático:** Buscar um artista e listar seus 5 álbuns mais recentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# Configure suas credenciais (obtidas no dashboard de desenvolvedor do Spotify)\n",
    "# É recomendado usar variáveis de ambiente para isso.\n",
    "client_id = \"SEU_CLIENT_ID\"\n",
    "client_secret = \"SEU_CLIENT_SECRET\"\n",
    "\n",
    "try:\n",
    "    auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "    sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "    # Busca pelo artista\n",
    "    results = sp.search(q='artist:Queen', type='artist')\n",
    "    items = results['artists']['items']\n",
    "\n",
    "    if len(items) > 0:\n",
    "        artist = items[0]\n",
    "        print(f\"Artista encontrado: {artist['name']}\")\n",
    "        \n",
    "        # Pega os álbuns do artista\n",
    "        albums_result = sp.artist_albums(artist['uri'], album_type='album', limit=5)\n",
    "        for album in albums_result['items']:\n",
    "            print(f\" - Álbum: {album['name']} ({album['release_date']})\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao acessar a API do Spotify. Verifique suas credenciais. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudo de Caso 3: Visão Geral de APIs do Twitter/X e Google\n",
    "\n",
    "* **Twitter/X:** A API do Twitter passou por mudanças significativas, tornando o acesso mais restrito e, em muitos casos, pago. Bibliotecas como `Twython` e `Tweepy` existem para facilitar a interação, mas é crucial verificar a documentação oficial da API do X para entender os níveis de acesso e custos atuais.\n",
    "* **Google:** O Google oferece uma vasta gama de APIs para seus produtos (Maps, YouTube, Drive, etc.). A biblioteca `google-api-python-client` é a ferramenta oficial para interagir com a maioria delas, gerenciando autenticação e a construção de requisições complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5: Tópicos Avançados e Estratégias de Robustez\n",
    "\n",
    "Para que um scraper seja útil em um ambiente de produção, ele precisa ser robusto, ou seja, capaz de lidar com logins, navegar por múltiplas páginas e evitar ser bloqueado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticação e Sessões com `requests.Session`\n",
    "\n",
    "Muitos sites exigem login para acessar determinados conteúdos. Fazer uma nova requisição de login a cada página seria ineficiente e provavelmente falharia, pois o estado da sessão (gerenciado por cookies) não seria mantido. O objeto `requests.Session` resolve isso. Ele funciona como um navegador persistente, armazenando cookies e cabeçalhos entre as requisições.[19, 66, 67]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código Prático:** Fluxo de login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de login e da página protegida\n",
    "login_url = 'http://quotes.toscrape.com/login'\n",
    "profile_url = 'http://quotes.toscrape.com/login' # A mesma página mostra o status de login\n",
    "\n",
    "# Inicia uma sessão\n",
    "with requests.Session() as session:\n",
    "    # Primeiro, obtemos o token CSRF da página de login\n",
    "    response = session.get(login_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    csrf_token = soup.find('input', {'name': 'csrf_token'})['value']\n",
    "    \n",
    "    # Dados para o POST de login\n",
    "    login_data = {\n",
    "        'username': 'admin', # Usuário de exemplo\n",
    "        'password': 'password', # Senha de exemplo\n",
    "        'csrf_token': csrf_token\n",
    "    }\n",
    "    \n",
    "    # Envia a requisição de login\n",
    "    session.post(login_url, data=login_data)\n",
    "    \n",
    "    # Agora, a sessão está autenticada. Podemos acessar páginas protegidas.\n",
    "    profile_page_response = session.get(profile_url)\n",
    "    \n",
    "    # Verificar se o login foi bem-sucedido\n",
    "    if \"Logout\" in profile_page_response.text:\n",
    "        print(\"Login realizado com sucesso!\")\n",
    "    else:\n",
    "        print(\"Falha no login.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estratégias Anti-Bloqueio\n",
    "\n",
    "Sites comerciais implementam sistemas anti-bot para proteger seus dados e infraestrutura. Um scraper que se comporta de forma muito diferente de um humano será rapidamente detectado e bloqueado.\n",
    "\n",
    "* **Rotação de User-Agents:** Como mencionado anteriormente, é crucial não usar o `User-Agent` padrão do `requests`. Uma prática ainda melhor é ter uma lista de `User-Agents` de navegadores reais e escolher um aleatoriamente para cada requisição, tornando o tráfego menos uniforme e mais difícil de ser identificado como automatizado.\n",
    "\n",
    "* **Uso de Proxies:** Fazer centenas ou milhares de requisições de um único endereço IP é o sinal mais claro de automação. Os proxies atuam como intermediários, fazendo com que suas requisições pareçam vir de diferentes locais e IPs. Existem vários tipos de proxies, como os de *datacenter* (rápidos, mas mais fáceis de detectar) e os *residenciais* (mais lentos, mas parecem tráfego de usuários reais, sendo mais difíceis de bloquear). A rotação de proxies a cada requisição ou após um certo número de requisições é uma técnica fundamental para scraping em larga escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de como usar um proxy com requests\n",
    "proxies = {\n",
    "   'http': 'http://10.10.1.10:3128', # Substitua pelo seu proxy\n",
    "   'https': 'https://10.10.1.10:1080', # Substitua pelo seu proxy\n",
    "}\n",
    "try:\n",
    "    requests.get('https://example.org', proxies=proxies, timeout=5)\n",
    "    print(\"Requisição com proxy (potencialmente) bem-sucedida.\")\n",
    "except requests.exceptions.ProxyError as e:\n",
    "    print(f\"Erro ao conectar ao proxy. Verifique o endereço. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistência de Dados\n",
    "\n",
    "Depois de extrair os dados, você precisa armazená-los de forma útil. Os formatos mais comuns são CSV (valores separados por vírgula), ideal para análise em planilhas ou com bibliotecas como Pandas, e JSON (JavaScript Object Notation), que é ótimo para estruturas de dados aninhadas e para ser consumido por outras aplicações.\n",
    "\n",
    "* **Salvando em CSV:** O módulo `csv` nativo do Python, especialmente a classe `DictWriter`, é perfeito para salvar uma lista de dicionários.\n",
    "* **Salvando em JSON:** O módulo `json` facilita a serialização de objetos Python (como listas e dicionários) para o formato JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 6: A Base de Tudo - Ética, Legalidade e Boas Práticas\n",
    "\n",
    "A funcionalidade técnica de um scraper é apenas metade da história. Um scraper bem-sucedido e sustentável é aquele que opera de forma ética e legal. Ignorar esses aspectos não só pode levar a problemas legais, mas também resulta em um scraper frágil, que é rapidamente bloqueado. As boas práticas éticas e as boas práticas de engenharia de software convergem: um scraper \"ético\" é, por definição, um scraper mais \"robusto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O `robots.txt`: O Protocolo de Exclusão de Robôs\n",
    "\n",
    "O arquivo `robots.txt` é um arquivo de texto que os administradores de sites colocam na raiz de seu domínio (ex: `https://www.example.com/robots.txt`) para dar instruções a robôs e crawlers. \n",
    "\n",
    "Ele especifica quais partes do site não devem ser acessadas (`Disallow`), quais podem ser (`Allow`) e, às vezes, uma taxa de rastreamento sugerida (`Crawl-delay`).\n",
    "\n",
    "Embora não seja tecnicamente obrigatório, respeitar o `robots.txt` é a primeira e mais importante regra da \"etiqueta de scraping\" e um sinal de respeito pela infraestrutura do site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navegando por Termos de Serviço e Direitos Autorais\n",
    "\n",
    "* **Termos de Serviço (ToS):** Sempre leia os Termos de Serviço de um site antes de fazer scraping. Muitos proíbem explicitamente a extração automatizada de dados. Violar os ToS pode ser considerado uma quebra de contrato e levar a consequências legais.\n",
    "* **Direitos Autorais (Copyright):** A lei de direitos autorais protege o conteúdo original. Enquanto extrair dados factuais (como preços de produtos, datas, nomes) é geralmente aceitável, copiar e republicar conteúdo criativo (como artigos de notícias, fotos, resenhas) sem permissão é uma violação de direitos autorais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leis de Proteção de Dados (LGPD/GDPR)\n",
    "\n",
    "Se o seu scraping envolve a coleta de qualquer Informação de Identificação Pessoal (PII) — como nomes, e-mails, números de telefone — de cidadãos, você deve estar ciente de leis rigorosas de proteção de dados como a LGPD no Brasil e a GDPR na Europa.\n",
    "\n",
    "Essas leis impõem restrições severas sobre como os dados pessoais podem ser coletados, processados e armazenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Etiqueta do Scraper: Evitando Sobrecarga\n",
    "\n",
    "A regra de ouro do scraping é: **não prejudique o site**. Um scraper agressivo pode sobrecarregar os servidores de um site, degradando a experiência para usuários humanos e potencialmente derrubando o serviço. Para ser um \"bom cidadão\" da web:\n",
    "* **Implemente Atrasos:** Adicione um atraso (`time.sleep()`) entre suas requisições para não bombardear o servidor.\n",
    "* **Limite a Concorrência:** Se estiver usando uma ferramenta como o `Scrapy`, configure-a para fazer um número razoável de requisições paralelas.\n",
    "* **Faça Scraping em Horários de Baixo Tráfego:** Se possível, execute seus scrapers durante a noite ou em horários de menor movimento no site.\n",
    "* **Use Cache:** Armazene as páginas que você já visitou. Se precisar re-executar o scraper, verifique seu cache primeiro para evitar fazer downloads repetidos e desnecessários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão: Escolhendo a Ferramenta Certa e Próximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Síntese e Guia de Decisão\n",
    "\n",
    "Neste guia, exploramos o vasto universo do web scraping com Python, desde a simplicidade de `requests` e `BeautifulSoup` para sites estáticos, passando pela complexidade da automação de navegadores com `Selenium` e `Playwright` para sites dinâmicos, até a escalabilidade do framework `Scrapy` para grandes projetos e a robustez do uso de APIs oficiais.\n",
    "\n",
    "A escolha da ferramenta correta não é uma questão de qual é \"a melhor\", mas sim de qual é a mais adequada para a tarefa em questão. A tabela a seguir serve como um guia de decisão rápido para seus futuros projetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cenário de Uso | Ferramenta Recomendada | Justificativa (Prós e Contras) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Site estático simples com poucas páginas** | `Requests` + `BeautifulSoup` | **Prós:** Extremamente rápido, baixo consumo de recursos, fácil de aprender. **Contras:** Não executa JavaScript. |\n",
    "| **Site moderno com JavaScript, SPAs, rolagem infinita** | `Playwright` (preferencial) ou `Selenium` | **Prós:** Executa JavaScript e simula interações humanas. `Playwright` tem melhor DX, `auto-waits` e performance. **Contras:** Lento e consome muitos recursos (CPU/RAM). |\n",
    "| **Extração de dados em larga escala de múltiplos sites** | `Scrapy` | **Prós:** Altamente performático (assíncrono), arquitetura organizada, \"batteries-included\" (pipelines, middlewares). **Contras:** Curva de aprendizado mais íngreme que bibliotecas simples. |\n",
    "| **Acesso a dados de uma grande plataforma (Reddit, Spotify, etc.)** | API Oficial + Biblioteca Wrapper (ex: `PRAW`, `Spotipy`) | **Prós:** Mais confiável, rápido, ético e legal. Dados já estruturados. **Contras:** Limitado aos dados que a API expõe e sujeito a rate limits. |\n",
    "| **Site dinâmico com API interna (descoberta via `Fetch/XHR`)** | `Requests` (após descoberta da API) | **Prós:** Combina a robustez de uma API com a capacidade de extrair dados de qualquer site. Extremamente eficiente. **Contras:** Requer uma etapa inicial de \"engenharia reversa\" para encontrar a API. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "MEU_AMBIENTE",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> a5e8c23b (Adicionando os notebooks da aula do dia 10/09, sobre coleta de dados via api e scrapt)
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.12"
=======
   "version": "3.12.7"
>>>>>>> a5e8c23b (Adicionando os notebooks da aula do dia 10/09, sobre coleta de dados via api e scrapt)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
