{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza e Transformação de Dados com Python\n",
    "\n",
    "## Seção 1: Introdução\n",
    "\n",
    "### 1.1 Ciência de Dados: Qualidade Acima de Tudo\n",
    "\n",
    "No cerne de toda análise de dados, modelo de machine learning ou sistema de inteligência artificial, reside um princípio imutável e fundamental: a qualidade da saída é inextricavelmente ligada à qualidade da entrada.\n",
    "- GIGO, ou \"Garbage In, Garbage Out\" (Lixo Entra, Lixo Sai).\n",
    "- Se os dados de entrada forem falhos, incompletos ou de baixa qualidade, os resultados gerados serão, na melhor das hipóteses, imprecisos e, na pior, perigosamente enganosos.\n",
    "\n",
    "Não é um fenômeno moderno.\n",
    " - **Século XIX: Charles Babbage sobre sua Máquina Diferencial.** Quando questionado se a máquina produziria a resposta correta ao ser alimentada com números errados, Babbage respondeu: \"Não sou capaz de apreender corretamente o tipo de confusão de ideias que poderia provocar tal pergunta\".\n",
    "\n",
    "Um sistema computacional não possui raciocínio independente; ele opera exclusivamente com base nos dados que lhe são fornecidos. \n",
    "\n",
    "A frase \"garbage in, garbage out\" foi popularizada nos primórdios da computação, por volta de 1957, possivelmente como uma gíria militar usada por engenheiros que trabalhavam com os primeiros computadores a válvulas, para lembrar que as máquinas apenas seguiam instruções e não podiam \"pensar\" por si mesmas.\n",
    "\n",
    "Dados de baixa qualidade podem levar a consequências comerciais e sociais significativas. \n",
    " - Previsões de demanda baseadas em dados imprecisos podem resultar em oportunidades de vendas perdidas ou excesso de estoque.\n",
    " - Campanhas de marketing direcionadas com base em dados demográficos desatualizados podem desperdiçar orçamentos consideráveis.\n",
    " - Falhas em cumprir regulamentações de privacidade, como o compartilhamento de informações sensíveis com as pessoas erradas, podem surgir de dados inconsistentes.\n",
    " - Modelos de IA generativa, se alimentados com informações falsas, irão, por sua vez, gerar desinformação com uma aparência de autoridade.\n",
    "\n",
    "Portanto, a limpeza e a preparação de dados não são meramente uma etapa preliminar, mas sim a fundação sobre a qual todo o projeto de ciência de dados é construído.\n",
    "\n",
    "### 1.2 O Que São \"Dados Sujos\"?\n",
    "\n",
    "Para combater eficazmente os \"dados sujos\", é crucial entender que o \"lixo\" se manifesta de várias formas.\n",
    "\n",
    "A qualidade dos dados pode ser avaliada através de várias dimensões principais, e uma falha em qualquer uma delas contribui para o problema do GIGO.\n",
    "\n",
    "Uma taxonomia do \"lixo\" de dados inclui:\n",
    "\n",
    "* **Imprecisão (Accuracy):** Os dados não correspondem à realidade. Um exemplo clássico são os erros de entrada de dados humanos. Em um contexto mais complexo, previsões iniciais sobre casos e mortalidade da COVID-19 foram superestimadas em algumas regiões porque os resultados de testes de fim de semana foram erroneamente combinados com os da semana seguinte, criando um pico artificial e, portanto, impreciso.\n",
    "* **Incompletude (Completeness):** Faltam informações. Campos deixados em branco em um formulário, leituras de sensores que falharam ou dados que não foram coletados para um subgrupo específico da população são exemplos de dados incompletos.\n",
    "* **Inconsistência (Consistency):** Os dados são contraditórios em diferentes sistemas ou mesmo dentro do mesmo conjunto de dados. Por exemplo, o estado da \"Louisiana\" pode ser registrado como \"Louisiana\" em um sistema de CRM, \"LA\" em um ERP e \"La.\" em um sistema de RH. Embora semanticamente corretos, esses formatos diferentes criam inconsistência que pode levar a registros duplicados ou análises incorretas.\n",
    "* **Falta de Pontualidade (Timeliness):** Os dados não estão disponíveis quando são necessários. Informações de endereço de entrega que chegam após o envio do pedido são inúteis para o sistema de atendimento de pedidos.\n",
    "* **Invalidade (Validity):** Os dados não estão em conformidade com as regras de negócio ou formatos predefinidos. Se um sistema exige um código postal de nove dígitos, um código de cinco dígitos inserido por um cliente seria considerado inválido.\n",
    "* **Falta de Unicidade (Uniqueness):** A mesma entidade ou evento é registrado várias vezes. Um cliente pode ter dois registros diferentes em um banco de dados, um para seu endereço residencial e outro para o comercial, em vez de um único registro com dois endereços associados.\n",
    "* **Viés (Bias):** Os dados não são representativos da população ou do fenômeno que se pretende modelar. Um sistema de IA para diagnóstico de doenças treinado predominantemente com dados de adultos será enviesado e provavelmente terá um desempenho ruim no diagnóstico de crianças. O viés nos dados de treinamento é uma das principais causas de resultados injustos e antiéticos em sistemas de IA.\n",
    "\n",
    "O \"lixo\" pode ir além dos próprios dados. O GIGO também pode se referir a \"pensamento falho\", como a aplicação de teorias incorretas, modelos conceituais errados, código mal documentado que leva a erros downstream, ou operações de pesquisa deficientes que resultam na coleta de dados errados. \n",
    "\n",
    "Limpeza de dados é um exercício de pensamento crítico.\n",
    " - Cientista de dados deve questionar não apenas a sintaxe dos dados, mas também sua semântica e relevância para o problema em questão.\n",
    "\n",
    "### 1.3 Estudo de Caso: O Dataset do Titanic\n",
    "\n",
    "Um dos conjuntos de dados mais icônicos e educativos da ciência de dados: o dataset do **Titanic** da plataforma Kaggle. \n",
    "\n",
    "Este dataset detalha o destino dos passageiros a bordo do RMS Titanic, que afundou em 15 de abril de 1912, resultando na morte de 1502 das 2224 pessoas a bordo. \n",
    "\n",
    "Ele serve como um laboratório perfeito por ser um conjunto de dados do mundo real, inerentemente \"sujo\", contendo valores ausentes, tipos de dados mistos e features que exigem engenharia para se tornarem úteis.\n",
    "\n",
    "[Dataset Titanic - Kaggle](https://www.kaggle.com/c/titanic)\n",
    "\n",
    "O primeiro passo é carregar o conjunto de dados de treinamento (`train.csv`) em um DataFrame do Pandas, a principal biblioteca de manipulação de dados em Python. Em seguida, realizaremos uma inspeção inicial para ter uma primeira impressão da estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 5 linhas do dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Últimas 5 linhas do dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.00</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.00</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "886          887         0       2                     Montvila, Rev. Juozas   \n",
       "887          888         1       1              Graham, Miss. Margaret Edith   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890         1       1                     Behr, Mr. Karl Howell   \n",
       "890          891         0       3                       Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
       "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
       "887  female  19.0      0      0      112053  30.00   B42        S  \n",
       "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
       "889    male  26.0      0      0      111369  30.00  C148        C  \n",
       "890    male  32.0      0      0      370376   7.75   NaN        Q  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando as bibliotecas pandas e numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o dataset de treinamento\n",
    "# Assumindo que o arquivo 'train.csv' está no mesmo diretório\n",
    "try:\n",
    "    df = pd.read_csv('../train.csv')\n",
    "    #df = pd.read_csv('titanic/train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo 'train.csv' não encontrado. Certifique-se de que ele está no diretório correto.\")\n",
    "    # Criando um dataframe vazio para o restante do código não quebrar\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Exibindo as 5 primeiras linhas do DataFrame\n",
    "print(\"Primeiras 5 linhas do dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Exibindo as 5 últimas linhas do DataFrame\n",
    "print(\"\\nÚltimas 5 linhas do dataset:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender o que cada coluna representa, é essencial consultar o dicionário de dados. Este documento fornece o contexto de domínio necessário para tomar decisões informadas durante o processo de limpeza.\n",
    "\n",
    "**Dicionário de Dados do Titanic:**\n",
    "\n",
    "| Variável | Definição | Chave/Valores |\n",
    "| :--- | :--- | :--- |\n",
    "| `Survived` | Sobrevivência | `0` = Não, `1` = Sim |\n",
    "| `Pclass` | Classe do bilhete | `1` = 1ª, `2` = 2ª, `3` = 3ª |\n",
    "| `Name` | Nome do passageiro | |\n",
    "| `Sex` | Sexo | `male`, `female` |\n",
    "| `Age` | Idade em anos | |\n",
    "| `SibSp` | Nº de irmãos/cônjuges a bordo | |\n",
    "| `Parch` | Nº de pais/filhos a bordo | |\n",
    "| `Ticket` | Número do bilhete | |\n",
    "| `Fare` | Tarifa do passageiro | |\n",
    "| `Cabin` | Número da cabine | |\n",
    "| `Embarked` | Porto de embarque | `C` = Cherbourg, `Q` = Queenstown, `S` = Southampton |\n",
    "\n",
    "Com o dataset carregado e o dicionário de dados em mãos, estamos prontos para iniciar o processo de diagnóstico e limpeza, transformando este conjunto de dados bruto em uma base sólida para análise e modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 2: Profiling Inicial do Dataset - Conhecendo Seu Inimigo\n",
    "\n",
    "Antes de aplicar qualquer técnica de limpeza, é imperativo realizar um perfilamento (profiling) completo do dataset. \n",
    "\n",
    "Esta fase de diagnóstico é análoga a um médico que realiza exames antes de prescrever um tratamento. \n",
    "\n",
    "O objetivo é obter uma compreensão profunda da estrutura, conteúdo e qualidade dos dados, identificando problemas potenciais que precisarão ser abordados.\n",
    "\n",
    "### 2.1 A Visão Geral: Estrutura e Metadados\n",
    "\n",
    "Iniciaremos com o método `.info()`. Ele fornece um resumo conciso e denso do DataFrame, incluindo o tipo de índice, nomes das colunas, contagem de valores não nulos e os tipos de dados (Dtypes) de cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações do DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Obtendo um resumo conciso do DataFrame\n",
    "print(\"Informações do DataFrame:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise da saída de `df.info()` para o dataset do Titanic revela imediatamente várias bandeiras vermelhas:\n",
    "* **Total de Entradas:** O `RangeIndex` mostra 891 entradas, o que significa que o dataset de treino tem 891 linhas (passageiros).\n",
    "* **Valores Ausentes:** A coluna `Non-Null Count` é a mais reveladora. Enquanto a maioria das colunas tem 891 valores não nulos, `Age` tem apenas 714, `Cabin` tem 204 e `Embarked` tem 889. Isso confirma a presença de dados ausentes e nos dá uma contagem exata dos valores presentes, permitindo-nos inferir os ausentes.\n",
    "* **Tipos de Dados:** A coluna `Dtype` mostra os tipos de dados. Vemos uma mistura de `int64` (inteiro), `float64` (ponto flutuante) e `object` (geralmente strings). Algumas colunas, como `Sex` e `Embarked`, são do tipo `object`, mas representam categorias, indicando a necessidade de uma futura conversão de tipo.\n",
    "\n",
    "Para obter as dimensões exatas do dataset (número de linhas e colunas) e uma lista limpa dos nomes das colunas, podemos usar os atributos `.shape` e `.columns`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as dimensões do DataFrame (linhas, colunas)\n",
    "print(f\"Dimensões do dataset: {df.shape}\")\n",
    "\n",
    "# Obtendo os nomes das colunas\n",
    "print(f\"Colunas do dataset: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 A Lupa nos Tipos de Dados (Dtypes)\n",
    "\n",
    "O tipo de dado de uma coluna (`dtype`) determina quais operações podem ser realizadas nela. Por exemplo, operações matemáticas podem ser aplicadas a colunas numéricas (`int64`, `float64`), mas não a colunas de texto (`object`). O método `.dtypes` fornece uma visão focada exclusivamente nos tipos de dados de cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os tipos de dados de cada coluna\n",
    "print(\"Tipos de dados (Dtypes):\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tipos de dados mais comuns no Pandas são:\n",
    "* `object`: O tipo mais geral, geralmente usado para armazenar strings, mas pode conter qualquer objeto Python.\n",
    "* `int64`: Inteiros de 64 bits.\n",
    "* `float64`: Números de ponto flutuante de 64 bits.\n",
    "* `bool`: Valores booleanos (True/False).\n",
    "* `datetime64[ns]`: Datas e horas, com precisão de nanossegundos.\n",
    "\n",
    "No dataset do Titanic, a análise dos `dtypes` sugere áreas para melhoria:\n",
    "* `Pclass`: É `int64`, mas representa uma categoria ordinal (1ª > 2ª > 3ª classe).\n",
    "* `Sex` e `Embarked`: São `object`, mas representam categorias nominais (sem ordem inerente).\n",
    "* `Survived`: É `int64`, mas representa uma categoria binária (0 ou 1).\n",
    "\n",
    "Corrigir esses tipos de dados não é apenas uma questão de \"boas práticas\"; é essencial para garantir que as análises e os modelos de machine learning interpretem essas variáveis corretamente.\n",
    "\n",
    "Uma técnica poderosa para trabalhar com DataFrames grandes é selecionar colunas programaticamente com base em seu tipo de dado, usando o método `.select_dtypes()`. Isso permite aplicar transformações em massa a todas as colunas numéricas ou a todas as colunas de texto, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas numéricas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare\n",
       "0            1         0       3  22.0      1      0   7.2500\n",
       "1            2         1       1  38.0      1      0  71.2833\n",
       "2            3         1       3  26.0      0      0   7.9250\n",
       "3            4         1       1  35.0      1      0  53.1000\n",
       "4            5         0       3  35.0      0      0   8.0500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colunas de objeto:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>113803</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>373450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name     Sex  \\\n",
       "0                            Braund, Mr. Owen Harris    male   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   \n",
       "2                             Heikkinen, Miss. Laina  female   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   \n",
       "4                           Allen, Mr. William Henry    male   \n",
       "\n",
       "             Ticket Cabin Embarked  \n",
       "0         A/5 21171   NaN        S  \n",
       "1          PC 17599   C85        C  \n",
       "2  STON/O2. 3101282   NaN        S  \n",
       "3            113803  C123        S  \n",
       "4            373450   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecionando apenas as colunas numéricas\n",
    "numeric_cols = df.select_dtypes(include=np.number)\n",
    "print(\"Colunas numéricas:\")\n",
    "print(type(numeric_cols))\n",
    "display(numeric_cols.head())\n",
    "\n",
    "# Selecionando apenas as colunas de objeto (texto)\n",
    "object_cols = df.select_dtypes(include='object')\n",
    "print(\"\\nColunas de objeto:\")\n",
    "display(object_cols.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Primeiras Pistas: Estatísticas Descritivas\n",
    "\n",
    "O método `.describe()` é uma ferramenta de diagnóstico fundamental que gera estatísticas descritivas para as colunas numéricas. Ele calcula medidas de tendência central (média, mediana), dispersão (desvio padrão) e a forma da distribuição (quartis, mínimo, máximo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatísticas descritivas (numéricas):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gerando estatísticas descritivas para colunas numéricas\n",
    "print(\"Estatísticas descritivas (numéricas):\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise cuidadosa desta tabela nos fornece pistas valiosas:\n",
    "* **`count`**: O valor para `Age` (714) é menor que o total de 891, confirmando novamente os valores ausentes de uma perspectiva estatística.\n",
    "* **`mean` vs. `50%` (mediana)**: Para a coluna `Age`, a média (29.7) e a mediana (28.0) são relativamente próximas, sugerindo uma distribuição razoavelmente simétrica. No entanto, para `Fare`, a média (32.2) é significativamente maior que a mediana (14.45), indicando uma forte assimetria à direita, provavelmente causada por valores extremamente altos.\n",
    "* **`max` vs. `75%`**: A discrepância mais gritante está na coluna `Fare`. O 75º percentil é 31.0, mas o valor máximo é 512.3. Isso é um forte indicador da presença de outliers, ou seja, tarifas muito mais altas que a da grande maioria dos passageiros.\n",
    "* **`min`**: A idade mínima (`Age`) é 0.42, o que corresponde a um bebê. Isso é consistente com o dicionário de dados, que menciona idades fracionárias para crianças com menos de um ano.\n",
    "\n",
    "Para as colunas categóricas (tipo `object`), podemos usar `.describe(include='object')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando estatísticas descritivas para colunas de objeto\n",
    "print(\"\\nEstatísticas descritivas (categóricas):\")\n",
    "display(df.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta saída nos informa que:\n",
    "* **`unique`**: Existem 891 nomes únicos (um por passageiro), mas apenas 2 valores únicos para `Sex` e 3 para `Embarked`. A coluna `Cabin` tem 147 valores únicos, sugerindo pouca reutilização ou muitos valores únicos para poucas entradas.\n",
    "* **`top`**: O sexo mais comum é `male`.\n",
    "* **`freq`**: `male` aparece 577 vezes. O porto de embarque mais comum é 'S' (Southampton), com 644 passageiros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 3: Tratamento de Dados Ausentes (Missing Values)\n",
    "\n",
    "Dados ausentes, frequentemente representados como `NaN` (Not a Number) no Pandas, são um dos problemas mais comuns e desafiadores na preparação de dados. \n",
    "\n",
    "Ignorá-los pode levar a erros em cálculos, análises enviesadas e modelos de machine learning com desempenho inferior. \n",
    "\n",
    "Abordar os dados ausentes requer uma estratégia deliberada, que equilibre a preservação da informação com a integridade do dataset.\n",
    "\n",
    "### 3.1 Quantificando o Problema\n",
    "\n",
    "O primeiro passo é obter uma contagem clara e precisa dos valores ausentes em cada coluna. \n",
    "\n",
    "A combinação dos métodos `.isnull()` (que retorna um DataFrame booleano indicando `True` para cada valor ausente) e `.sum()` (que, em um DataFrame booleano, conta o número de `True`s) é a maneira padrão e mais eficiente de fazer isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass   Name    Sex    Age  SibSp  Parch  Ticket  \\\n",
      "0          False     False   False  False  False  False  False  False   False   \n",
      "1          False     False   False  False  False  False  False  False   False   \n",
      "2          False     False   False  False  False  False  False  False   False   \n",
      "3          False     False   False  False  False  False  False  False   False   \n",
      "4          False     False   False  False  False  False  False  False   False   \n",
      "..           ...       ...     ...    ...    ...    ...    ...    ...     ...   \n",
      "886        False     False   False  False  False  False  False  False   False   \n",
      "887        False     False   False  False  False  False  False  False   False   \n",
      "888        False     False   False  False  False   True  False  False   False   \n",
      "889        False     False   False  False  False  False  False  False   False   \n",
      "890        False     False   False  False  False  False  False  False   False   \n",
      "\n",
      "      Fare  Cabin  Embarked  \n",
      "0    False   True     False  \n",
      "1    False  False     False  \n",
      "2    False   True     False  \n",
      "3    False  False     False  \n",
      "4    False   True     False  \n",
      "..     ...    ...       ...  \n",
      "886  False   True     False  \n",
      "887  False  False     False  \n",
      "888  False   True     False  \n",
      "889  False  False     False  \n",
      "890  False   True     False  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "Contagem de valores ausentes por coluna:\n",
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contando o número de valores nulos (NaN) em cada coluna\n",
    "missing_values = df.isnull().sum()\n",
    "print(df.isnull())\n",
    "print(\"Contagem de valores ausentes por coluna:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora a contagem absoluta seja útil, calcular a *porcentagem* de valores ausentes geralmente fornece um contexto melhor para a tomada de decisões. Uma coluna com 1000 valores ausentes em um dataset de um milhão de linhas é muito menos problemática do que uma com 50 valores ausentes em um dataset de 100 linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porcentagem de valores ausentes por coluna:\n",
      "Cabin       77.104377\n",
      "Age         19.865320\n",
      "Embarked     0.224467\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculando a porcentagem de valores ausentes por coluna\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPorcentagem de valores ausentes por coluna:\")\n",
    "print(missing_percentage[missing_percentage > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o dataset do Titanic, os resultados confirmam nossas observações iniciais:\n",
    "* **`Cabin`**: Aproximadamente 77% dos valores estão ausentes. Este é um nível extremamente alto, tornando qualquer forma de imputação muito especulativa e potencialmente prejudicial.\n",
    "* **`Age`**: Cerca de 20% dos valores estão ausentes. Esta é uma quantidade significativa, mas não proibitiva. A remoção desses dados seria imprudente, pois perderíamos 20% de nossas observações. A imputação é a abordagem mais provável aqui.\n",
    "* **`Embarked`**: Apenas 2 valores estão ausentes (cerca de 0.22%). Este número é tão pequeno que a remoção das linhas correspondentes teria um impacto mínimo no dataset geral.\n",
    "\n",
    "### 3.2 Estratégia 1: Remoção (A Ferramenta Cega)\n",
    "\n",
    "A estratégia mais direta para lidar com dados ausentes é simplesmente removê-los. O Pandas oferece duas maneiras principais de fazer isso: remover linhas (`.dropna()`) ou remover colunas (`.drop()`).\n",
    "\n",
    "#### Remoção de Linhas\n",
    "\n",
    "É apropriado remover linhas quando a quantidade de dados ausentes é muito pequena e distribuída aleatoriamente, de modo que a perda de informação é insignificante e não introduz viés na amostra.\n",
    "\n",
    "**Caso de Uso (Titanic):** As duas linhas onde `Embarked` está ausente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato original: (891, 12)\n",
      "Formato após remover linhas com 'Embarked' ausente: (889, 12)\n"
     ]
    }
   ],
   "source": [
    "# Criando uma cópia para demonstração\n",
    "df_dropped_rows = df.copy()\n",
    "\n",
    "# Removendo linhas onde 'Embarked' é nulo\n",
    "# O argumento 'subset' especifica que a verificação de NaN deve ser feita apenas nesta coluna INPLACE, garante que sera removido do mesmo dataFrame\n",
    "df_dropped_rows.dropna(subset=['Embarked'], inplace=True)\n",
    "\n",
    "\n",
    "print(f\"Formato original: {df.shape}\")\n",
    "print(f\"Formato após remover linhas com 'Embarked' ausente: {df_dropped_rows.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de Colunas\n",
    "\n",
    "A remoção de uma coluna inteira é uma medida drástica, justificada apenas quando a coluna tem uma porcentagem esmagadora de valores ausentes, a ponto de não conter informações úteis.\n",
    "\n",
    "**Caso de Uso (Titanic):** A coluna `Cabin`. Com 77% de ausência, tentar preencher esses valores seria, em grande parte, fabricar dados. É mais seguro e honesto remover a coluna e reconhecer que essa informação não está disponível para a maioria dos passageiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas originais: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "Colunas após remover 'Cabin': ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# Criando uma cópia para demonstração\n",
    "df_dropped_cols = df.copy()\n",
    "\n",
    "# Removendo a coluna 'Cabin'\n",
    "# O argumento 'axis=1' especifica que estamos removendo uma coluna, não uma linha\n",
    "df_dropped_cols.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "print(f\"Colunas originais: {df.columns.tolist()}\")\n",
    "print(f\"Colunas após remover 'Cabin': {df_dropped_cols.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Estratégia 2: Imputação (A Arte da Estimação)\n",
    "\n",
    "A imputação é o processo de preencher valores ausentes com valores estimados. É uma abordagem mais sofisticada que preserva o tamanho do dataset. A escolha do método de imputação depende do tipo de dado (numérico ou categórico) e de sua distribuição. A principal ferramenta para isso no Pandas é o método `.fillna()`.\n",
    "\n",
    "#### Imputação Numérica para 'Age'\n",
    "\n",
    "Para a coluna `Age`, que é numérica, as opções mais comuns são a média e a mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de idade: 29.70\n",
      "Mediana de idade: 28.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_mean_imputed</th>\n",
       "      <th>Age_median_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54.0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Age_mean_imputed  Age_median_imputed\n",
       "0  22.0         22.000000                22.0\n",
       "1  38.0         38.000000                38.0\n",
       "2  26.0         26.000000                26.0\n",
       "3  35.0         35.000000                35.0\n",
       "4  35.0         35.000000                35.0\n",
       "5   NaN         29.699118                28.0\n",
       "6  54.0         54.000000                54.0\n",
       "7   2.0          2.000000                 2.0\n",
       "8  27.0         27.000000                27.0\n",
       "9  14.0         14.000000                14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imputação pela Média\n",
    "# Calculando a média da idade (ignorando NaNs por padrão)\n",
    "mean_age = df['Age'].mean()\n",
    "print(f\"Média de idade: {mean_age:.2f}\")\n",
    "\n",
    "# Preenchendo valores ausentes com a média (em uma nova coluna para comparação)\n",
    "df['Age_mean_imputed'] = df['Age'].fillna(mean_age)\n",
    "\n",
    "# Imputação pela Mediana\n",
    "# Calculando a mediana da idade\n",
    "median_age = df['Age'].median()\n",
    "print(f\"Mediana de idade: {median_age:.2f}\")\n",
    "\n",
    "# Preenchendo valores ausentes com a mediana (em uma nova coluna para comparação)\n",
    "df['Age_median_imputed'] = df['Age'].fillna(median_age)\n",
    "\n",
    "display(df[['Age', 'Age_mean_imputed', 'Age_median_imputed']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputação Categórica para 'Embarked'\n",
    "\n",
    "Para dados categóricos, a imputação com a média ou mediana não faz sentido. A estratégia padrão é usar a moda, que é o valor mais frequente na coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porto de embarque mais comum (moda): S\n",
      "Valores ausentes em 'Embarked_mode_imputed': 0\n"
     ]
    }
   ],
   "source": [
    "# Calculando a moda de 'Embarked'\n",
    "# .mode() retorna uma Series, então pegamos o primeiro elemento com [0]\n",
    "mode_embarked = df['Embarked'].mode()[0]\n",
    "print(f\"Porto de embarque mais comum (moda): {mode_embarked}\")\n",
    "\n",
    "# Preenchendo os 2 valores ausentes com a moda\n",
    "df['Embarked_mode_imputed'] = df['Embarked'].fillna(mode_embarked)\n",
    "\n",
    "# Verificando se os valores ausentes foram preenchidos\n",
    "print(f\"Valores ausentes em 'Embarked_mode_imputed': {df['Embarked_mode_imputed'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Técnica | Descrição | Prós | Contras | Quando Usar |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Remoção de Linhas (`dropna`) | Exclui qualquer linha que contenha um valor ausente. | Simples, rápido. | Perda de dados, pode introduzir viés. | Quando a quantidade de dados ausentes é muito pequena (<1-2%). |\n",
    "| Remoção de Colunas (`drop`) | Exclui uma coluna inteira. | Remove features problemáticas. | Perda total da informação da feature. | Quando a coluna tem uma porcentagem muito alta de valores ausentes (>50-60%). |\n",
    "| Imputação por Média | Preenche com o valor médio da coluna. | Simples, mantém o tamanho do dataset. | Sensível a outliers, reduz a variância. | Dados numéricos com distribuição simétrica e poucos outliers. |\n",
    "| Imputação por Mediana | Preenche com o valor mediano da coluna. | Robusto a outliers. | Pode alterar a distribuição original. | Dados numéricos com distribuição assimétrica ou com outliers. |\n",
    "| Imputação por Moda | Preenche com o valor mais frequente. | Simples, aplicável a dados categóricos. | Pode criar um viés para a categoria modal. | Dados categóricos. |\n",
    "\n",
    "### Exercício 1\n",
    "\n",
    "\n",
    "1.  Compare as estatísticas descritivas (`.describe()`) das três colunas ('Age', 'Age_mean_imputed', 'Age_median_imputed'). A imputação alterou significativamente o desvio padrão (`std`)? Por que isso acontece?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'oloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jl/02kt03gj5n99h1czkswp582c0000gn/T/ipykernel_4807/2719925965.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Repositórios/Ci-ncia_de_dados/Meu_Ambiente/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6314\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6316\u001b[0m         ):\n\u001b[1;32m   6317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'oloc'"
     ]
    }
   ],
   "source": [
    "df.oloc[\"Age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 4: Lidando com Dados Duplicados\n",
    "\n",
    "Dados duplicados podem inflar artificialmente o tamanho do dataset, distorcer estatísticas e levar a modelos de machine learning que atribuem peso indevido a observações repetidas. \n",
    "\n",
    "A identificação e o tratamento de duplicatas são passos essenciais para garantir a integridade e a unicidade dos dados.\n",
    "\n",
    "### 4.1 Identificação de Duplicatas\n",
    "\n",
    "O Pandas fornece o método `.duplicated()` para identificar linhas duplicadas. Por padrão, ele retorna uma Série booleana onde `True` marca uma linha que é uma cópia exata de uma linha que apareceu *anteriormente* no DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame de exemplo com duplicatas\n",
    "data_com_duplicatas = {\n",
    "    'Nome': ['Ana', 'Bruno', 'Carlos', 'Ana', 'Daniel', 'Bruno'],\n",
    "    'Idade': [28, 35, 22, 28, 40, 35]\n",
    "}\n",
    "df_dups = pd.DataFrame(data_com_duplicatas)\n",
    "\n",
    "print(\"DataFrame de exemplo:\")\n",
    "display(df_dups)\n",
    "\n",
    "# Identificando linhas duplicadas (mantendo a primeira ocorrência como não-duplicata)\n",
    "print(\"\\nSérie booleana de duplicatas (keep='first'):\")\n",
    "print(df_dups.duplicated())\n",
    "\n",
    "# Contando o número total de linhas duplicadas\n",
    "num_duplicatas = df_dups.duplicated().sum()\n",
    "print(f\"\\nNúmero de linhas duplicadas: {num_duplicatas}\")\n",
    "\n",
    "# Exibindo as linhas que são duplicatas\n",
    "print(\"\\nLinhas duplicadas:\")\n",
    "display(df_dups[df_dups.duplicated()])\n",
    "\n",
    "# Identificando todas as ocorrências de linhas duplicadas\n",
    "print(\"\\nTodas as linhas envolvidas em duplicação (keep=False):\")\n",
    "display(df_dups[df_dups.duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Remoção e Estratégias\n",
    "\n",
    "#### Duplicatas Exatas\n",
    "\n",
    "Este é o caso mais simples, onde removemos linhas que são idênticas em todas as colunas. Vamos verificar isso no dataset do Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando duplicatas exatas no dataset do Titanic\n",
    "titanic_dups_count = df.duplicated().sum()\n",
    "print(f\"Número de linhas completamente duplicadas no dataset do Titanic: {titanic_dups_count}\")\n",
    "\n",
    "# Se houvesse duplicatas, poderíamos removê-las com:\n",
    "# df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicatas Parciais\n",
    "\n",
    "Um cenário mais comum e sutil é a duplicação baseada em um subconjunto de colunas. Por exemplo, podemos querer verificar se há passageiros com o mesmo nome e idade. Isso é feito usando o parâmetro `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando duplicatas com base em 'Name' e 'Age'\n",
    "partial_dups = df.duplicated(subset=['Name', 'Age']).sum()\n",
    "print(f\"Número de duplicatas com base em Nome e Idade: {partial_dups}\")\n",
    "\n",
    "# Exibindo as duplicatas parciais (se houver)\n",
    "if partial_dups > 0:\n",
    "    display(df[df.duplicated(subset=['Name', 'Age'], keep=False)].sort_values(by='Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregação em Vez de Remoção\n",
    "\n",
    "A verdadeira tarefa ao lidar com duplicatas é garantir a *granularidade* correta do dataset. A granularidade define o que cada linha representa. Para o dataset do Titanic, a granularidade é \"um passageiro por linha\". Em outros contextos, a remoção pode ser a abordagem errada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo hipotético de agregação de dados de vendas\n",
    "sales_data = {\n",
    "    'CustomerID': [101, 102, 101, 103, 102, 101],\n",
    "    'Amount': [50, 100, 25, 200, 75, 10]\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Agregando para obter o gasto total e o número de compras por cliente\n",
    "customer_summary = df_sales.groupby('CustomerID').agg(\n",
    "    TotalAmount=('Amount', 'sum'),\n",
    "    PurchaseCount=('Amount', 'count')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Dataset de vendas original:\")\n",
    "display(df_sales)\n",
    "print(\"\\nDataset agregado por cliente:\")\n",
    "display(customer_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: Garantindo a Unicidade\n",
    "\n",
    "1.  Verifique se existem duplicatas completas no dataset `train.csv` do Titanic.\n",
    "2.  Verifique se existe algum passageiro com o mesmo 'Ticket' e o mesmo 'Cabin'. Isso necessariamente significa um erro de dados? Discuta o que isso poderia significar.\n",
    "3.  Considere o DataFrame de vendas do exemplo acima. Como você encontraria o valor médio de compra por cliente? (Dica: use `.groupby()` e o método de agregação apropriado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5: Tratamento de Outliers\n",
    "\n",
    "Outliers são pontos de dados que se desviam significativamente do resto do conjunto de dados. \n",
    "\n",
    "Eles podem ser resultado de erros de medição, erros de entrada de dados ou eventos genuinamente raros. \n",
    "\n",
    "Independentemente da sua origem, os outliers podem ter um impacto desproporcional em análises estatísticas e no treinamento de modelos de machine learning.\n",
    "\n",
    "### 5.1 O Que São Outliers e Por Que se Preocupar?\n",
    "\n",
    "Um outlier é uma observação que se encontra a uma distância anormal de outros valores em uma amostra aleatória de uma população. A presença de outliers pode:\n",
    "* **Distorcer Estatísticas Descritivas**\n",
    "* **Impactar o Desempenho do Modelo**\n",
    "* **Violar Suposições dos Modelos**\n",
    "\n",
    "### 5.2 Detecção Visual: O Poder do Box Plot\n",
    "\n",
    "Uma das maneiras mais eficazes e intuitivas de detectar outliers é através da visualização. O box plot (ou diagrama de caixa) é uma ferramenta gráfica poderosa para essa finalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurando o estilo dos gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Box plot para 'Age'\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df['Age'])\n",
    "plt.title('Box Plot da Idade (Age)')\n",
    "plt.ylabel('Idade')\n",
    "\n",
    "# Box plot para 'Fare'\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['Fare'])\n",
    "plt.title('Box Plot da Tarifa (Fare)')\n",
    "plt.ylabel('Tarifa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualização confirma nossas suspeitas da análise descritiva:\n",
    "* **`Age`**: Possui alguns outliers na extremidade superior, mas a distribuição geral parece contida.\n",
    "* **`Fare`**: Apresenta um grande número de outliers na extremidade superior. A maioria dos dados está concentrada em valores baixos, mas há uma longa cauda de valores extremamente altos, incluindo os pontos acima de 500 que havíamos notado.\n",
    "\n",
    "### 5.3 Detecção Quantitativa: O Método IQR\n",
    "\n",
    "O box plot nos dá uma visão qualitativa. Para identificar outliers de forma programática, podemos usar a mesma lógica quantitativa por trás das whiskers: o método IQR.\n",
    "\n",
    "A regra é: um valor é considerado um outlier se estiver abaixo de $Q1 - 1.5 \\times IQR$ ou acima de $Q3 + 1.5 \\times IQR$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando Q1, Q3 e IQR para a coluna 'Fare'\n",
    "Q1 = df['Fare'].quantile(0.25)\n",
    "Q3 = df['Fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definindo os limites para detecção de outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Primeiro Quartil (Q1): {Q1:.2f}\")\n",
    "print(f\"Terceiro Quartil (Q3): {Q3:.2f}\")\n",
    "print(f\"Intervalo Interquartil (IQR): {IQR:.2f}\")\n",
    "print(f\"Limite Inferior para Outliers: {lower_bound:.2f}\")\n",
    "print(f\"Limite Superior para Outliers: {upper_bound:.2f}\")\n",
    "\n",
    "# Identificando os outliers\n",
    "outliers = df[(df['Fare'] < lower_bound) | (df['Fare'] > upper_bound)]\n",
    "\n",
    "print(f\"\\nNúmero de outliers encontrados em 'Fare': {len(outliers)}\")\n",
    "print(\"Exemplos de outliers em 'Fare':\")\n",
    "display(outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Estratégias de Tratamento de Outliers\n",
    "\n",
    "Vamos demonstrar a técnica de **capping** para a coluna `Fare`, que consiste em limitar os valores outliers a um valor máximo/mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia do DataFrame para não alterar o original\n",
    "df_capped = df.copy()\n",
    "\n",
    "# Aplicando o capping na coluna 'Fare'\n",
    "df_capped['Fare_capped'] = np.where(\n",
    "    df_capped['Fare'] > upper_bound,\n",
    "    upper_bound,\n",
    "    df_capped['Fare']\n",
    ")\n",
    "\n",
    "# Comparando as estatísticas e os box plots antes e depois do capping\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df['Fare'])\n",
    "plt.title('Box Plot Original de Fare')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df_capped['Fare_capped'])\n",
    "plt.title('Box Plot de Fare Após Capping')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Estatísticas descritivas de 'Fare' original:\")\n",
    "display(df['Fare'].describe())\n",
    "print(\"\\nEstatísticas descritivas de 'Fare' com capping:\")\n",
    "display(df_capped['Fare_capped'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Técnica | Descrição | Prós | Contras | Quando Usar |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Remoção | Exclui as linhas que contêm valores outliers. | Simples de implementar. | Perda de informação; pode introduzir viés. | Quando os outliers são claramente erros de dados e em pequeno número. |\n",
    "| Capping | Limita os valores outliers a um teto (e/ou piso) predefinido. | Preserva a observação no dataset; mitiga a influência do valor extremo. | A escolha do limite é subjetiva; pode distorcer a distribuição. | Quando se deseja manter a observação, mas reduzir o impacto de valores extremos em modelos sensíveis a eles. |\n",
    "| Transformação (ex: Log) | Aplica uma função matemática (como logaritmo) para comprimir a escala. | Reduz a assimetria e o impacto dos outliers; pode ajudar a satisfazer suposições de modelos. | Torna a interpretação dos coeficientes do modelo menos direta. | Para dados com forte assimetria à direita, como dados financeiros ou contagens. |\n",
    "| Ignorar | Não fazer nada e usar modelos robustos a outliers. | Mantém a integridade original dos dados. | Requer o uso de algoritmos específicos (ex: árvores de decisão, RobustScaler). | Quando os outliers são eventos genuínos e importantes, e o modelo escolhido pode lidar com eles (ex: Random Forest). |\n",
    "\n",
    "### Exercício 3: Caça aos Outliers\n",
    "\n",
    "1.  Usando o método IQR, calcule os limites inferior e superior para outliers na coluna 'Age' do dataset Titanic.\n",
    "2.  Quantos outliers de idade existem no dataset? Eles são mais velhos ou mais novos que o esperado?\n",
    "3.  Crie uma nova coluna chamada 'Age_capped' onde você aplica a técnica de capping para os outliers de idade que você encontrou.\n",
    "4.  Gere box plots lado a lado para 'Age' e 'Age_capped' para visualizar o efeito do capping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 6: Limpeza e Padronização de Dados Textuais\n",
    "\n",
    "Dados textuais, ou de string, são frequentemente uma fonte de \"lixo\" devido à sua natureza não estruturada. Inconsistências na capitalização, espaços em branco indesejados e a presença de caracteres especiais podem fazer com que valores que são semanticamente idênticos sejam tratados como diferentes por um computador.\n",
    "\n",
    "### 6.1 A Desordem nos Dados de Texto\n",
    "\n",
    "Problemas comuns incluem:\n",
    "- Capitalização inconsistente\n",
    "- Espaços em branco\n",
    "- Caracteres especiais\n",
    "- Formatos variados\n",
    "\n",
    "### 6.2 Ferramentas Essenciais: O Acessor `.str` do Pandas\n",
    "\n",
    "O Pandas oferece um conjunto poderoso de métodos para manipulação de strings, acessíveis através do acessor `.str` em uma Série. Isso permite aplicar funções de string a cada elemento da Série de forma vetorizada e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma Série de exemplo com texto sujo\n",
    "cidades_sujas = pd.Series(['  Rio de Janeiro ', 'são PAULO', 'belo horizonte   ', 'SALVADOR'])\n",
    "\n",
    "print(\"Série original:\")\n",
    "print(cidades_sujas)\n",
    "\n",
    "# Aplicando uma cadeia de métodos de limpeza\n",
    "# .title() capitaliza a primeira letra de cada palavra\n",
    "cidades_limpas = cidades_sujas.str.strip().str.title()\n",
    "\n",
    "print(\"\\nSérie após limpeza:\")\n",
    "print(cidades_limpas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 O Poder das Expressões Regulares (Regex)\n",
    "\n",
    "Para tarefas de limpeza mais complexas, como remover caracteres específicos ou extrair padrões de texto, podemos usar **expressões regulares (regex)**. \n",
    "\n",
    "Um caso de uso prático no dataset do Titanic é extrair o título de cada passageiro (Mr., Mrs., Miss., etc.) da coluna `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraindo o título da coluna 'Name' usando regex\n",
    "# O método .str.extract() é projetado para extrair grupos de captura de uma regex\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "\n",
    "print(\"Títulos extraídos:\")\n",
    "display(df[['Name', 'Title']].head())\n",
    "\n",
    "print(\"\\nContagem de cada título:\")\n",
    "print(df['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4: Polindo o Texto\n",
    "\n",
    "1.  Crie uma nova coluna no DataFrame do Titanic chamada `Sex_cleaned`, que seja uma versão da coluna `Sex` com todas as letras em maiúsculas e sem espaços em branco no início ou no fim (embora o dataset original já esteja limpo, pratique a aplicação dos métodos).\n",
    "2.  Na coluna `Ticket`, alguns valores contêm texto e números (ex: 'A/5 21171'), enquanto outros são puramente numéricos. Use `.str.replace()` com uma expressão regular para remover todos os caracteres não numéricos da coluna `Ticket`, criando uma nova coluna `Ticket_num`. (Dica: a regex `[^0-9]` corresponde a qualquer caractere que *não* seja um dígito).\n",
    "3.  Observe os títulos que você extraiu no exemplo. Alguns, como `Mlle` (Mademoiselle) e `Ms` são variações de `Miss`. `Mme` (Madame) é uma variação de `Mrs`. Use o método `.replace()` (não o `.str.replace()`) para padronizar esses títulos na coluna `Title`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 7: Transformação e Engenharia de Features (Feature Engineering)\n",
    "\n",
    "Após a limpeza básica dos dados, o próximo passo é a transformação e a engenharia de features. Este é um processo criativo e orientado pelo domínio que envolve a modificação de features existentes e a criação de novas para melhorar o desempenho do modelo de machine learning.\n",
    "\n",
    "### 7.1 Correção de Tipos de Dados\n",
    "\n",
    "Como identificado na fase de perfilamento, várias colunas no dataset do Titanic têm tipos de dados que não refletem sua natureza lógica. Corrigir isso é um passo fundamental da transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para as transformações\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Corrigindo o tipo de Pclass para 'category'\n",
    "df_transformed['Pclass'] = df_transformed['Pclass'].astype('category')\n",
    "print(\"Dtype de Pclass após a conversão:\")\n",
    "print(df_transformed.dtypes['Pclass'])\n",
    "\n",
    "# Exemplo de uso de pd.to_datetime\n",
    "date_series = pd.Series(['2023-01-01', '02/01/2023', 'Jan 3, 2023', 'invalid_date'])\n",
    "\n",
    "# Convertendo para datetime, com erros sendo transformados em NaT (Not a Time)\n",
    "converted_dates = pd.to_datetime(date_series, format=\"mixed\",errors='coerce')\n",
    "print(\"\\nDatas convertidas:\")\n",
    "print(converted_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Binning: Convertendo Contínuo em Categórico\n",
    "\n",
    "Binning (ou discretização) é o processo de transformar uma variável numérica contínua em uma variável categórica. Vamos usar `pd.cut()` para criar grupos de idade no dataset do Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os limites (bordas) dos bins de idade\n",
    "# -1 é usado para garantir que a idade 0 seja incluída no primeiro bin\n",
    "age_bins = [-1, 12, 18, 60, 100] # Bins: 0-12, 13-18, 19-60, 61-100\n",
    "age_labels = ['Criança', 'Adolescente', 'Adulto', 'Idoso']\n",
    "\n",
    "# Aplicando pd.cut para criar a nova coluna categórica\n",
    "# Usando a coluna 'Age_median_imputed' que já tratamos\n",
    "df_transformed['AgeGroup'] = pd.cut(df_transformed['Age_median_imputed'], bins=age_bins, labels=age_labels, right=True)\n",
    "\n",
    "print(\"Distribuição dos grupos de idade:\")\n",
    "print(df_transformed['AgeGroup'].value_counts())\n",
    "display(df_transformed[['Age_median_imputed', 'AgeGroup']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Extração de Features a Partir de Colunas Existentes\n",
    "\n",
    "Esta é a essência da engenharia de features: combinar ou decompor variáveis existentes para criar novas que sejam mais informativas.\n",
    "\n",
    "#### Criando `FamilySize` e `IsAlone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a feature FamilySize\n",
    "df_transformed['FamilySize'] = df_transformed['SibSp'] + df_transformed['Parch'] + 1 # +1 para contar a própria pessoa\n",
    "\n",
    "# Criando a feature IsAlone\n",
    "df_transformed['IsAlone'] = 0\n",
    "df_transformed.loc[df_transformed['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "print(\"Distribuição de IsAlone:\")\n",
    "print(df_transformed['IsAlone'].value_counts())\n",
    "display(df_transformed[['FamilySize', 'IsAlone']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refinando a feature `Title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando títulos raros em uma única categoria 'Rare'\n",
    "rare_titles = df_transformed['Title'].value_counts()[df_transformed['Title'].value_counts() < 10].index\n",
    "df_transformed['Title'] = df_transformed['Title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "# Padronizando alguns títulos\n",
    "df_transformed['Title'] = df_transformed['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n",
    "\n",
    "print(\"\\nContagem de títulos após agrupamento:\")\n",
    "print(df_transformed['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4: Criando Novas Variáveis\n",
    "\n",
    "1.  Use `pd.qcut()` para dividir a coluna 'Fare' em 4 quantis (quartis). Dê os nomes 'Muito Baixa', 'Baixa', 'Média', 'Alta' para os bins. Armazene o resultado em uma nova coluna chamada 'Fare_quantile'.\n",
    "2.  A primeira letra do número da cabine (`Cabin`) pode indicar a localização no navio (Deck A, B, C, etc.). Embora a coluna `Cabin` tenha muitos valores ausentes, crie uma nova coluna `Deck` extraindo a primeira letra dos valores não nulos de `Cabin`. Preencha os valores ausentes com 'U' (de Unknown). (Dica: use `.str` para pegar o primeiro caractere e `.fillna('U')`).\n",
    "3.  Crie uma feature binária chamada `Is_Mr` que seja 1 se o `Title` do passageiro for 'Mr' e 0 caso contrário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 8: Codificação de Variáveis Categóricas\n",
    "\n",
    "A maioria dos algoritmos de machine learning, como regressão logística, SVMs e redes neurais, opera com base em matemática e, portanto, exige que todas as features de entrada sejam numéricas. Variáveis categóricas, como 'Sex' (`male`, `female`) ou 'Embarked' (`S`, `C`, `Q`), precisam ser convertidas em uma representação numérica. Esse processo é chamado de codificação.\n",
    "\n",
    "### 8.1 Label Encoding: Para Dados Ordinais\n",
    "\n",
    "Label Encoding (ou Codificação de Rótulos) atribui um número inteiro único a cada categoria. Esta técnica é apropriada **apenas** para **dados ordinais**, onde existe uma ordem ou ranking inerente entre as categorias. Para a coluna `Sex`, que é binária, o Label Encoding é aceitável e pode ser feito facilmente com o método `.map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para a codificação\n",
    "df_encoded = df_transformed.copy()\n",
    "\n",
    "# Aplicando Label Encoding na coluna 'Sex'\n",
    "df_encoded['Sex'] = df_encoded['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "display(df_encoded[['Name', 'Sex']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 One-Hot Encoding: Para Dados Nominais\n",
    "\n",
    "Para dados nominais (sem ordem inerente), a técnica correta é o One-Hot Encoding. Este método cria novas colunas binárias (0 ou 1) para cada categoria única na variável original. A principal ferramenta para isso no Pandas é a função `pd.get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos a coluna de Embarked já imputada\n",
    "df_encoded['Embarked'] = df_encoded['Embarked_mode_imputed']\n",
    "\n",
    "# Aplicando One-Hot Encoding na coluna 'Embarked'\n",
    "embarked_dummies = pd.get_dummies(df_encoded['Embarked'], prefix='Embarked', drop_first=True)\n",
    "title_dummies = pd.get_dummies(df_encoded['Title'], prefix='Title', drop_first=True)\n",
    "\n",
    "# Juntando as novas colunas ao DataFrame principal e removendo as originais\n",
    "df_encoded = pd.concat([df_encoded, embarked_dummies, title_dummies], axis=1)\n",
    "df_encoded.drop(['Embarked', 'Title'], axis=1, inplace=True)\n",
    "\n",
    "print(\"\\nColunas do DataFrame após codificar 'Sex', 'Embarked' e 'Title':\")\n",
    "print(df_encoded.columns.tolist())\n",
    "display(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Técnica | Tipo de Dado | Descrição | Prós | Contras |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Label Encoding | Ordinal | Mapeia cada categoria para um inteiro único (0, 1, 2...). | Simples, não aumenta a dimensionalidade. | Introduz uma ordem artificial e falsa se usada em dados nominais. |\n",
    "| One-Hot Encoding | Nominal | Cria uma nova coluna binária (0/1) para cada categoria. | Não assume nenhuma ordem; funciona bem com a maioria dos algoritmos. | Aumenta a dimensionalidade (pode ser um problema com muitas categorias); Causa multicolinearidade (resolvido com `drop_first=True`). |\n",
    "\n",
    "### Exercício 7: Traduzindo Categorias\n",
    "\n",
    "1.  Aplique o One-Hot Encoding à coluna `Pclass`. Use o argumento `prefix` para nomear as novas colunas como 'Classe_1', 'Classe_2', etc. Não use `drop_first` desta vez.\n",
    "2.  Qual método de codificação você usaria para a coluna `AgeGroup` que criamos anteriormente? Justifique sua resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 9: Escalonamento de Features Numéricas\n",
    "\n",
    "Após a limpeza e codificação, as features numéricas do nosso dataset, como 'Age' e 'Fare', podem existir em escalas muito diferentes. O escalonamento de features é o processo de transformar as features numéricas para que todas estejam na mesma escala, garantindo que cada uma contribua de forma mais equitativa para o resultado do modelo.\n",
    "\n",
    "### 9.1 Normalização (Min-Max Scaling)\n",
    "\n",
    "A normalização, especificamente a Min-Max Scaling, redimensiona os dados para um intervalo fixo, geralmente entre 0 e 1. É sensível a outliers.\n",
    "\n",
    "$$ x_{scaled}= \\frac{x-x_{min}}{x_{max}-x_{min}} $$\n",
    "\n",
    "Onde $x_{min}$ e $x_{max}$ são os valores mínimo e máximo da feature, respectivamente.\n",
    "\n",
    "* Vantagem: preserva a forma da distribuição original.\n",
    "* Desvantagem: sensibilidade a outliers. Como a escala é definida pelos valores mínimo e máximo, um único outlier extremo pode comprimir todos os outros dados em um intervalo muito pequeno, distorcendo a escala.\n",
    "  \n",
    "O MinMaxScaler é frequentemente usado em algoritmos como redes neurais, que esperam valores de entrada em um intervalo pequeno e limitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Selecionando as colunas numéricas para escalonar\n",
    "# Usaremos as versões já tratadas\n",
    "numeric_features = ['Age_median_imputed', 'Fare']\n",
    "df_to_scale = df_encoded[numeric_features].copy()\n",
    "\n",
    "# Inicializando o scaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Ajustando e transformando os dados\n",
    "df_minmax_scaled = scaler_minmax.fit_transform(df_to_scale)\n",
    "\n",
    "# Convertendo o resultado de volta para um DataFrame para visualização\n",
    "df_minmax_scaled = pd.DataFrame(df_minmax_scaled, columns=numeric_features)\n",
    "\n",
    "print(\"Dados antes da Normalização (Min-Max Scaling):\")\n",
    "display(df_to_scale.describe())\n",
    "print(\"\\nDados após a Normalização (Min-Max Scaling):\")\n",
    "display(df_minmax_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Padronização (Standard Scaling)\n",
    "\n",
    "A padronização, ou Standard Scaling, transforma os dados para que eles tenham uma média de 0 e um desvio padrão de 1. É menos sensível a outliers e é a técnica de escalonamento mais comum.\n",
    "\n",
    "$$ x_{scaled}= \\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "Onde μ é a média da feature e σ é o seu desvio padrão. O valor resultante é frequentemente chamado de Z-score.\n",
    "\n",
    "A principal vantagem do StandardScaler é que ele é menos sensível a outliers do que o MinMaxScaler. Embora os outliers ainda influenciem o cálculo da média e do desvio padrão, seu efeito é amortecido e não define os limites da escala. A padronização não limita os valores a um intervalo específico.   \n",
    "\n",
    "O StandardScaler é a técnica de escalonamento mais comum e é preferida para algoritmos que assumem que os dados são normalmente distribuídos (ou pelo menos têm uma distribuição Gaussiana), como modelos lineares, Regressão Logística e Análise de Componentes Principais (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Inicializando o scaler\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Ajustando e transformando os dados\n",
    "df_std_scaled = scaler_std.fit_transform(df_to_scale)\n",
    "\n",
    "# Convertendo o resultado de volta para um DataFrame para visualização\n",
    "df_std_scaled = pd.DataFrame(df_std_scaled, columns=numeric_features)\n",
    "\n",
    "print(\"Dados antes da Padronização (Standard Scaling):\")\n",
    "display(df_to_scale.describe())\n",
    "print(\"\\nDados após a Padronização (Standard Scaling):\")\n",
    "display(df_std_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Qual Usar? Um Guia Prático\n",
    "\n",
    "* **Use `MinMaxScaler` (Normalização) quando:**\n",
    "    * O algoritmo não faz suposições sobre a distribuição dos dados (ex: KNN, Redes Neurais).\n",
    "    * Você precisa que os dados estejam em um intervalo específico (ex: 0-1).\n",
    "    * Seus dados têm poucos ou nenhum outlier.\n",
    "* **Use `StandardScaler` (Padronização) quando:**\n",
    "    * O algoritmo assume que os dados são normalmente distribuídos (ex: Regressão Linear/Logística).\n",
    "    * Seus dados contêm outliers.\n",
    "    * É a escolha padrão e mais segura na maioria dos cenários.\n",
    "\n",
    "### Exercício 8: Nivelando o Campo de Jogo\n",
    "\n",
    "1.  Crie um DataFrame simples com duas colunas: `A = [1, 2, 3, 4, 100]` e `B = [10, 20, 30, 40, 50]`.\n",
    "2.  Aplique o `MinMaxScaler` a este DataFrame. Observe os valores escalados da coluna `A`. Como o outlier (100) afetou a escala dos outros valores?\n",
    "3.  Agora, aplique o `StandardScaler` ao mesmo DataFrame original. Compare os valores escalados da coluna `A` com os do passo anterior. A padronização pareceu mais ou menos afetada pelo outlier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 10: Estruturando um Pipeline de Limpeza em um Notebook\n",
    "\n",
    "A solução para um fluxo de trabalho de limpeza confuso é passar de um estilo exploratório para um mais estruturado, encapsulando a lógica em funções reutilizáveis.\n",
    "\n",
    "### 10.1 Boas Práticas em Notebooks Jupyter\n",
    "\n",
    "Para manter a clareza e a reprodutibilidade, é recomendável adotar algumas boas práticas:\n",
    "\n",
    "* **Modularidade**: Em vez de ter longas sequências de código em células individuais, agrupe as etapas de limpeza em funções. Crie funções para tarefas específicas, como `handle_missing_values(df)`, `create_features(df)`, `encode_categoricals(df)`, etc..\n",
    "* **Encapsulamento**: Combine essas funções menores em uma única função mestre de pré-processamento, como `preprocess_data(df)`. Esta função servirá como um pipeline completo, recebendo um DataFrame bruto e retornando um DataFrame limpo e pronto para a modelagem.\n",
    "* **Separação Lógica**: Estruture seu notebook em seções claras: 1. Carregamento de Dados, 2. Definição das Funções de Limpeza, 3. Aplicação do Pipeline, 4. Análise Exploratória (no dataset limpo), 5. Modelagem.\n",
    "* **Evite Modificações In-Place**: Sempre que possível, evite usar inplace=True. Em vez disso, faça com que suas funções retornem um novo DataFrame modificado. Isso torna o fluxo de dados mais explícito e evita efeitos colaterais inesperados.\n",
    "\n",
    "### 10.2 Exemplo de um Pipeline de Limpeza Completo\n",
    "\n",
    "Vamos agora consolidar todas as etapas de limpeza e transformação que discutimos ao longo desta aula em uma única função `preprocess_titanic`. Esta função encapsulará toda a lógica e poderá ser aplicada de forma idêntica aos conjuntos de treino e teste, garantindo consistência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_titanic(df, scaler=None, is_train=True):\n",
    "    \"\"\"\n",
    "    Aplica um pipeline completo de pré-processamento a um DataFrame do Titanic.\n",
    "    \"\"\"\n",
    "    # 1. Copiando o DataFrame para evitar modificar o original\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # 2. Tratamento de Dados Ausentes\n",
    "    processed_df.drop('Cabin', axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    if 'Age' in processed_df.columns:\n",
    "        median_age = processed_df['Age'].median()\n",
    "        processed_df['Age'].fillna(median_age, inplace=True)\n",
    "    \n",
    "    if 'Embarked' in processed_df.columns:\n",
    "        mode_embarked = processed_df['Embarked'].mode()[0]\n",
    "        processed_df['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "    \n",
    "    if 'Fare' in processed_df.columns:\n",
    "        mean_fare = processed_df['Fare'].mean()\n",
    "        processed_df['Fare'].fillna(mean_fare, inplace=True)\n",
    "\n",
    "    # 3. Engenharia de Features\n",
    "    processed_df['FamilySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1\n",
    "    processed_df['IsAlone'] = 0\n",
    "    processed_df.loc[processed_df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    processed_df['Title'] = processed_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    processed_df['Title'] = processed_df['Title'].fillna('Unknown')\n",
    "    rare_titles = processed_df['Title'].value_counts()[processed_df['Title'].value_counts() < 10].index\n",
    "    processed_df['Title'] = processed_df['Title'].replace(rare_titles, 'Rare')\n",
    "    processed_df['Title'] = processed_df['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n",
    "    \n",
    "    # 4. Codificação de Variáveis Categóricas\n",
    "    processed_df['Sex'] = processed_df['Sex'].map({'male': 0, 'female': 1})\n",
    "    processed_df = pd.get_dummies(processed_df, columns=['Embarked', 'Title'], drop_first=True)\n",
    "\n",
    "    # 5. Tratamento de Outliers (Capping em 'Fare')\n",
    "    Q1 = processed_df['Fare'].quantile(0.25)\n",
    "    Q3 = processed_df['Fare'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    processed_df['Fare'] = np.where(processed_df['Fare'] > upper_bound, upper_bound, processed_df['Fare'])\n",
    "\n",
    "    # 6. Remoção de Colunas Desnecessárias\n",
    "    processed_df.drop(['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # 7. Escalonamento de Features Numéricas\n",
    "    numeric_features_to_scale = ['Age', 'Fare', 'FamilySize']\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        processed_df[numeric_features_to_scale] = scaler.fit_transform(processed_df[numeric_features_to_scale])\n",
    "        return processed_df, scaler\n",
    "    else:\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"Scaler deve ser fornecido para o conjunto de teste.\")\n",
    "        processed_df[numeric_features_to_scale] = scaler.transform(processed_df[numeric_features_to_scale])\n",
    "        return processed_df\n",
    "\n",
    "# Carregando os dados de treino e teste\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "\n",
    "    # Aplicando o pipeline\n",
    "    # Note que o scaler ajustado no treino é passado para transformar o teste\n",
    "    train_clean, fitted_scaler = preprocess_titanic(train_df, is_train=True)\n",
    "    test_clean = preprocess_titanic(test_df, scaler=fitted_scaler, is_train=False)\n",
    "\n",
    "    print(\"Dataset de treino limpo:\")\n",
    "    display(train_clean.head())\n",
    "    print(f\"\\nFormato do treino limpo: {train_clean.shape}\")\n",
    "\n",
    "    print(\"\\nDataset de teste limpo:\")\n",
    "    display(test_clean.head())\n",
    "    print(f\"\\nFormato do teste limpo: {test_clean.shape}\")\n",
    "\n",
    "    # Verificando se há valores nulos restantes\n",
    "    print(\"\\nValores nulos no treino limpo:\", train_clean.isnull().sum().sum())\n",
    "    print(\"Valores nulos no teste limpo:\", test_clean.isnull().sum().sum())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivos 'train.csv' ou 'test.csv' não encontrados. Pule a execução do pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 11: Conclusão - A Jornada Contínua da Qualidade de Dados\n",
    "\n",
    "### 11.1 Recapitulando o Processo\n",
    "\n",
    "Ao longo desta aula, navegamos pela jornada essencial de transformar dados brutos e caóticos em um formato limpo, estruturado e pronto para análise. Este processo, longe de ser uma tarefa trivial, é a espinha dorsal de qualquer projeto de ciência de dados bem-sucedido. Recapitulamos as etapas críticas que constituem um pipeline de pré-processamento robusto:\n",
    "\n",
    "1.  **Profiling Inicial**\n",
    "2.  **Tratamento de Dados Ausentes**\n",
    "3.  **Gerenciamento de Duplicatas**\n",
    "4.  **Detecção e Tratamento de Outliers**\n",
    "5.  **Limpeza de Texto**\n",
    "6.  **Engenharia e Transformação de Features**\n",
    "7.  **Codificação Categórica**\n",
    "8.  **Escalonamento de Features**\n",
    "\n",
    "A culminação de todas essas etapas foi a construção de um pipeline de pré-processing encapsulado em uma única função, uma prática que promove a reprodutibilidade, a consistência e a eficiência.\n",
    "\n",
    "### 11.2 Limpeza de Dados como um Processo Iterativo\n",
    "\n",
    "É fundamental reconhecer que a limpeza de dados raramente é um processo linear, executado uma única vez. Na prática, é um ciclo iterativo, intimamente entrelaçado com a Análise Exploratória de Dados (EDA) e a modelagem. \n",
    "\n",
    "A visualização dos dados limpos pode revelar novos padrões ou problemas que exigem um retorno às etapas de pré-processamento. \n",
    "\n",
    "Da mesma forma, os resultados de um modelo inicial podem indicar que certas features não são úteis ou que uma transformação diferente poderia ser mais eficaz.\n",
    "\n",
    "Portanto, a mentalidade correta não é \"limpar e esquecer\", mas sim \"limpar, analisar, modelar, refinar\".\n",
    "\n",
    "### 11.3 Conclusão\n",
    "\n",
    "Em conclusão, a preparação de dados é tanto uma arte quanto uma ciência. Ela exige rigor técnico, pensamento crítico e uma compreensão profunda do contexto do problema. \n",
    "\n",
    "A habilidade de transformar dados brutos e caóticos em clareza e insight é, sem dúvida, uma das competências mais valiosas de um cientista de dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Meu_Ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
